{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import models\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper import *\n",
    "from ground_truth import create_GT_masks\n",
    "from UV_mapping import create_UV_XYZ_dictionary\n",
    "from LineMOD import LineMODDataset\n",
    "from PoseRefinement import PoseRefinerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_correspondence_block(root_dir, classes, epochs=20, batch_size = 3):\n",
    "\n",
    "    train_data = LineMODDataset(root_dir, classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)]))\n",
    "\n",
    "    \n",
    "    num_workers = 0\n",
    "    valid_size = 0.2\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    # architecture for correspondence block - 13 objects + backgound = 14 channels for ID masks\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "\n",
    "    # custom loss function and optimizer\n",
    "    criterion_id = nn.CrossEntropyLoss()\n",
    "    criterion_u = nn.CrossEntropyLoss()\n",
    "    criterion_v = nn.CrossEntropyLoss()\n",
    "\n",
    "    # specify optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        correspondence_block.parameters(), lr=3e-4, weight_decay=3e-5)\n",
    "\n",
    "    # training loop\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = epochs\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "    \n",
    "    correspondance_train_loss_history = [] # track correspondance loss history\n",
    "    correspondance_valid_loss_history = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        torch.cuda.empty_cache()\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        print(\"------ Epoch \", epoch, \" ---------\")\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        correspondence_block.train()\n",
    "        for _, image, idmask, umask, vmask in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            image, idmask, umask, vmask = image.cuda(\n",
    "            ), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
    "            # calculate the batch loss\n",
    "            loss_id = criterion_id(idmask_pred, idmask)\n",
    "            loss_u = criterion_u(umask_pred, umask)\n",
    "            loss_v = criterion_v(vmask_pred, vmask)\n",
    "            loss = loss_id + loss_u + loss_v\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        correspondence_block.eval()\n",
    "        for _, image, idmask, umask, vmask in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            image, idmask, umask, vmask = image.cuda(\n",
    "            ), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
    "            # calculate the batch loss\n",
    "            loss_id = criterion_id(idmask_pred, idmask)\n",
    "            loss_u = criterion_u(umask_pred, umask)\n",
    "            loss_v = criterion_v(vmask_pred, vmask)\n",
    "            loss = loss_id + loss_u + loss_v\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "        correspondance_train_loss_history.append(train_loss)\n",
    "        correspondance_valid_loss_history.append(valid_loss)\n",
    "        \n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            torch.save(correspondence_block.state_dict(),\n",
    "                       'correspondence_block.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    print(\"correspondance_train_loss_history\", correspondance_train_loss_history)\n",
    "    print(\"correspondance_valid_loss_history\", correspondance_valid_loss_history)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(correspondance_train_loss_history, label = \"Training Loss\")\n",
    "    plt.plot(correspondance_valid_loss_history, label = \"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    Upscaling then double conv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels = 3, out_channels_id = 9, out_channels_uv = 256, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.out_channels_id = out_channels_id\n",
    "        self.out_channels_uv = out_channels_uv\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024//factor)\n",
    "\n",
    "\n",
    "        #ID MASK\n",
    "        self.up1_id = Up(1024, 512, bilinear)\n",
    "        self.up2_id = Up(512, 256, bilinear)\n",
    "        self.up3_id = Up(256, 128, bilinear)\n",
    "        self.up4_id = Up(128, 64 * factor, bilinear)\n",
    "        self.outc_id = OutConv(64, out_channels_id)\n",
    "\n",
    "        #U Mask\n",
    "        self.up1_u = Up(1024, 512, bilinear)\n",
    "        self.up2_u = Up(512,512,bilinear)\n",
    "        self.outc_u1 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u2 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u3 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u4 = OutConv(256, out_channels_uv)\n",
    "        self.up3_u = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4_u = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        #V Mask\n",
    "        self.up1_v = Up(1024, 512, bilinear)\n",
    "        self.up2_v = Up(512,512,bilinear)\n",
    "        self.outc_v1 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v2 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v3 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v4 = OutConv(256, out_channels_uv)\n",
    "        self.up3_v = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4_v = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # ID mask\n",
    "        x_id = self.up1_id(x5, x4)\n",
    "        x_id = self.up2_id(x_id, x3)\n",
    "        x_id = self.up3_id(x_id, x2)\n",
    "        x_id = self.up4_id(x_id, x1)\n",
    "        logits_id = self.outc_id(x_id)\n",
    "\n",
    "        # U mask\n",
    "        x_u = self.up1_u(x5, x4)\n",
    "        x_u = self.up2_u(x_u,x3)\n",
    "        x_u = self.outc_u1(x_u)\n",
    "        x_u = self.outc_u2(x_u)\n",
    "        x_u = self.outc_u3(x_u)\n",
    "        x_u = self.up3_u(x_u)\n",
    "        x_u = self.up4_u(x_u)\n",
    "        logits_u = self.outc_u4(x_u)\n",
    "\n",
    "        # V mask\n",
    "        x_v = self.up1_v(x5, x4)\n",
    "        x_v = self.up2_v(x_v,x3)\n",
    "        x_v = self.outc_v1(x_v)\n",
    "        x_v = self.outc_v2(x_v)\n",
    "        x_v = self.outc_v3(x_v)\n",
    "        x_v = self.up3_v(x_v)\n",
    "        x_v = self.up4_v(x_v)\n",
    "        logits_v = self.outc_v4(x_v)\n",
    "        \n",
    "        return logits_id,logits_u, logits_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_pose_estimation(root_dir, classes, intrinsic_matrix):\n",
    "\n",
    "    # LineMOD Dataset\n",
    "    train_data = LineMODDataset(root_dir, classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    # load the best correspondence block weights\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "    correspondence_block.load_state_dict(torch.load(\n",
    "        'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "    # initial 6D pose prediction\n",
    "    regex = re.compile(r'\\d+')\n",
    "    outliers = 0\n",
    "    for i in range(len(train_data)):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n",
    "        img_adr, img, idmask, _, _ = train_data[i]\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "        idmask_pred, umask_pred, vmask_pred = correspondence_block(img.cuda())\n",
    "        # convert the masks to 240,320 shape\n",
    "        temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "        upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
    "        vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
    "        coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
    "\n",
    "        adr = root_dir + label + \"/predicted_pose/\" + \\\n",
    "            \"info_\" + str(idx) + \".txt\"\n",
    "\n",
    "        coord_2d = torch.cat((coord_2d[0].view(\n",
    "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
    "        dct_keys = tuple(dct_keys.numpy())\n",
    "        dct = load_obj(root_dir + label + \"/UV-XYZ_mapping\")\n",
    "        mapping_2d = []\n",
    "        mapping_3d = []\n",
    "        for count, (u, v) in enumerate(dct_keys):\n",
    "            if (u, v) in dct:\n",
    "                mapping_2d.append(np.array(coord_2d[count]))\n",
    "                mapping_3d.append(dct[(u, v)])\n",
    "        # Get the 6D pose from rotation and translation matrices\n",
    "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
    "        if len(mapping_2d) >= 4 or len(mapping_3d) >= 4:\n",
    "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
    "                                                          np.array(mapping_2d, dtype=np.float32), \n",
    "                                                          intrinsic_matrix, \n",
    "                                                          distCoeffs=None,\n",
    "                                                          iterationsCount=150, \n",
    "                                                          reprojectionError=1.0, \n",
    "                                                          flags=cv2.SOLVEPNP_P3P)\n",
    "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
    "            rot[np.isnan(rot)] = 1\n",
    "            tvecs[np.isnan(tvecs)] = 1\n",
    "            tvecs = np.where(-100 < tvecs, tvecs, np.array([-100.]))\n",
    "            tvecs = np.where(tvecs < 100, tvecs, np.array([100.]))\n",
    "            rot_tra = np.append(rot, tvecs, axis=1)\n",
    "            # save the predicted pose\n",
    "            np.savetxt(adr, rot_tra)\n",
    "        else:  # save a pose full of zeros\n",
    "            outliers += 1\n",
    "            rot_tra = np.ones((3, 4))\n",
    "            rot_tra[:, 3] = 0\n",
    "            np.savetxt(adr, rot_tra)\n",
    "    print(\"Number of instances where PnP couldn't be used: \", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_refinement_inputs(root_dir, classes, intrinsic_matrix):\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "    correspondence_block.load_state_dict(torch.load(\n",
    "        'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "    train_data = LineMODDataset(root_dir, \n",
    "                                classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    upsampled = nn.Upsample(size=[240, 320], mode='bilinear',align_corners=False)\n",
    "\n",
    "    regex = re.compile(r'\\d+')\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n",
    "        img_adr, img, _, _, _ = train_data[i]\n",
    "\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        adr_rendered = root_dir + label + \\\n",
    "            \"/pose_refinement/rendered/color\" + str(idx) + \".png\"\n",
    "        adr_img = root_dir + label + \\\n",
    "            \"/pose_refinement/real/color\" + str(idx) + \".png\"\n",
    "        # find the object in the image using the idmask\n",
    "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "        idmask_pred, _, _ = correspondence_block(img.cuda())\n",
    "        idmask = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "        coord_2d = (idmask == classes[label]).nonzero(as_tuple=True)\n",
    "        if coord_2d[0].nelement() != 0:\n",
    "            coord_2d = torch.cat((coord_2d[0].view(\n",
    "                coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "            min_x = coord_2d[:, 0].min()\n",
    "            max_x = coord_2d[:, 0].max()\n",
    "            min_y = coord_2d[:, 1].min()\n",
    "            max_y = coord_2d[:, 1].max()\n",
    "            img = img.squeeze().transpose(1, 2).transpose(0, 2)\n",
    "            obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
    "            # saving in the correct format using upsampling\n",
    "            obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
    "            obj_img = upsampled(obj_img)\n",
    "            obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "            mpimg.imsave(adr_img, obj_img.squeeze().numpy())\n",
    "\n",
    "            # create rendering for an object\n",
    "            cropped_rendered_image = create_rendering(\n",
    "                root_dir, intrinsic_matrix, label, idx)\n",
    "            rendered_img = torch.from_numpy(cropped_rendered_image)\n",
    "            rendered_img = rendered_img.unsqueeze(dim=0)\n",
    "            rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
    "            rendered_img = upsampled(rendered_img)\n",
    "            rendered_img = rendered_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "            mpimg.imsave(adr_rendered, rendered_img.numpy())\n",
    "\n",
    "        else:  # object not present in idmask prediction\n",
    "            count += 1\n",
    "            mpimg.imsave(adr_rendered, np.zeros((240, 320)))\n",
    "            mpimg.imsave(adr_img, np.zeros((240, 320)))\n",
    "    print(\"Number of outliers: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pose_Refiner(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Pose_Refiner, self).__init__()\n",
    "        self.feature_extractor_image = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
    "                                                                           progress=True).children())[:9])\n",
    "        self.feature_extractor_rendered = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
    "                                                                              progress=True).children())[:9])\n",
    "        self.fc_xyhead_1 = nn.Linear(512, 253)\n",
    "        self.fc_xyhead_2 = nn.Linear(256, 2)\n",
    "        self.fc_zhead = nn.Sequential(nn.Linear(512, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(256, 1))\n",
    "        self.fc_Rhead_1 = nn.Linear(512, 252)\n",
    "        self.fc_Rhead_2 = nn.Linear(256, 4)\n",
    "\n",
    "        self.relu_layer = nn.ReLU()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # weight initialization\n",
    "        nn.init.constant_(self.fc_xyhead_1.weight, 0.)\n",
    "        nn.init.constant_(self.fc_xyhead_1.bias, 0.)\n",
    "\n",
    "        weights = torch.zeros((2, 256))\n",
    "        weights[0, 253] = torch.tensor(1.)\n",
    "        weights[1, 254] = torch.tensor(1.)\n",
    "        self.fc_xyhead_2.weight = nn.Parameter(weights)\n",
    "        nn.init.constant_(self.fc_xyhead_2.bias, 0.)\n",
    "\n",
    "        nn.init.constant_(self.fc_zhead.weight, 0.)\n",
    "        nn.init.constant_(self.fc_zhead.bias, 0.)\n",
    "\n",
    "        nn.init.constant_(self.fc_Rhead_1.weight, 0.)\n",
    "        nn.init.constant_(self.fc_Rhead_1.bias, 0.)\n",
    "\n",
    "        rand_weights = torch.zeros((4, 256))\n",
    "        rand_weights[0, 252] = torch.tensor(1.)\n",
    "        rand_weights[1, 253] = torch.tensor(1.)\n",
    "        rand_weights[2, 254] = torch.tensor(1.)\n",
    "        rand_weights[3, 255] = torch.tensor(1.)\n",
    "        self.fc_Rhead_2.weight = nn.Parameter(weights)\n",
    "        nn.init.constant_(self.fc_Rhead_2.bias, 0.)\n",
    "\n",
    "    def forward(self, image, rendered, pred_pose, bs=1):\n",
    "        # extracting the feature vector f\n",
    "        f_image = self.feature_extractor_image(image)\n",
    "        f_rendered = self.feature_extractor_rendered(rendered)\n",
    "        f_image = f_image.view(bs, -1)\n",
    "        f_image = self.relu_layer(f_image)\n",
    "        f_rendered = f_rendered.view(bs, -1)\n",
    "        f_rendered = self.relu_layer(f_rendered)\n",
    "        f = f_image - f_rendered\n",
    "\n",
    "        # Z refinement head\n",
    "        z = self.fc_zhead(f)\n",
    "\n",
    "        # XY refinement head\n",
    "        f_xy1 = self.fc_xyhead_1(f)\n",
    "        f_xy1 = self.relu_layer(f_xy1)\n",
    "        x_pred = np.reshape(pred_pose[:, 0, 3], (bs, -1))\n",
    "        y_pred = np.reshape(pred_pose[:, 1, 3], (bs, -1))\n",
    "        f_xy1 = torch.cat((f_xy1, x_pred.float().cuda()), 1)\n",
    "        f_xy1 = torch.cat((f_xy1, y_pred.float().cuda()), 1)\n",
    "        f_xy1 = torch.cat((f_xy1, z), 1)\n",
    "        xy = self.fc_xyhead_2(f_xy1.cuda())\n",
    "\n",
    "        # Rotation head\n",
    "        f_r1 = self.fc_Rhead_1(f)\n",
    "        f_r1 = self.relu_layer(f_r1)\n",
    "        r = R.from_matrix(pred_pose[:, 0:3, 0:3])\n",
    "        r = r.as_quat()\n",
    "        r = np.reshape(r, (bs, -1))\n",
    "        f_r1 = torch.cat(\n",
    "            (f_r1, torch.from_numpy(r).float().cuda()), 1)\n",
    "        rot = self.fc_Rhead_2(f_r1)\n",
    "\n",
    "        return xy, z, rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pose_refinement(root_dir, classes, epochs=5):\n",
    "    \n",
    "    train_data = PoseRefinerDataset(root_dir, classes=classes,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.ToPILImage(mode=None),\n",
    "                                        transforms.Resize(size=(224, 224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [\n",
    "                                                             0.229, 0.224, 0.225]),\n",
    "                                        transforms.ColorJitter(\n",
    "                                            brightness=0, contrast=0, saturation=0, hue=0)\n",
    "                                    ]))\n",
    "\n",
    "    pose_refiner = Pose_Refiner()\n",
    "    pose_refiner.cuda()\n",
    "    # freeze resnet\n",
    "    # pose_refiner.feature_extractor[0].weight.requires_grad = False\n",
    "\n",
    "    batch_size = 2\n",
    "    num_workers = 0\n",
    "    valid_size = 0.2\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    optimizer = optim.Adam(pose_refiner.parameters(),\n",
    "                           lr=1.5e-4, weight_decay=3e-5)\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = epochs\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"----- Epoch Number: \", epoch, \"--------\")\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        pose_refiner.train()\n",
    "        for label, image, rendered, true_pose, pred_pose in train_loader:\n",
    "            # move tensors to GPU\n",
    "            image, rendered = image.cuda(), rendered.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
    "            # convert rot quarternion to rotational matrix\n",
    "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "            rot[rot == float(\"Inf\")] = 1\n",
    "            xy[torch.isnan(xy)] == 0\n",
    "            z[torch.isnan(z)] == 0\n",
    "\n",
    "            rot = torch.tensor(\n",
    "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
    "            # update predicted pose\n",
    "            pred_pose[:, 0:3, 0:3] = rot\n",
    "            pred_pose[:, 0, 3] = xy[:, 0]\n",
    "            pred_pose[:, 1, 3] = xy[:, 1]\n",
    "            pred_pose[:, 2, 3] = z.squeeze()\n",
    "            # fetch point cloud data\n",
    "            pt_cld = fetch_ptcld_data(root_dir, label, batch_size)\n",
    "            # calculate the batch loss\n",
    "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        pose_refiner.eval()\n",
    "        for label, image, rendered, true_pose, pred_pose in valid_loader:\n",
    "            # move tensors to GPU\n",
    "            image, rendered = image.cuda(), rendered.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
    "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "            rot[rot == float(\"Inf\")] = 1            \n",
    "            xy[torch.isnan(xy)] == 0\n",
    "            z[torch.isnan(z)] == 0\n",
    "            # convert R quarternion to rotational matrix\n",
    "            rot = torch.tensor(\n",
    "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
    "            # update predicted pose\n",
    "            pred_pose[:, 0:3, 0:3] = rot\n",
    "            pred_pose[:, 0, 3] = xy[:, 0]\n",
    "            pred_pose[:, 1, 3] = xy[:, 1]\n",
    "            pred_pose[:, 2, 3] = z.squeeze()\n",
    "            # fetch point cloud data\n",
    "            pt_cld = fetch_ptcld_data(root_dir, label, batch_size)\n",
    "            # calculate the batch loss\n",
    "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min, valid_loss))\n",
    "            torch.save(pose_refiner.state_dict(), 'pose_refiner.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "def fetch_ptcld_data(root_dir, label, bs):\n",
    "    # detch pt cld data for batchsize\n",
    "    pt_cld_data = []\n",
    "    for i in range(bs):\n",
    "        obj_dir = root_dir + label[i] + \"/object.xyz\"\n",
    "        pt_cld = np.loadtxt(obj_dir, skiprows=1, usecols=(0, 1, 2))\n",
    "        index = np.random.choice(pt_cld.shape[0], 3000, replace=False)\n",
    "        pt_cld_data.append(pt_cld[index, :])\n",
    "    pt_cld_data = np.stack(pt_cld_data, axis=0)\n",
    "    return pt_cld_data\n",
    "\n",
    "\n",
    "# no. of points is always 3000\n",
    "def Matching_loss(pt_cld_rand, true_pose, pred_pose, bs, training=True):\n",
    "\n",
    "    total_loss = torch.tensor([0.])\n",
    "    total_loss.requires_grad = True\n",
    "    for i in range(0, bs):\n",
    "        pt_cld = pt_cld_rand[i, :, :].squeeze()\n",
    "        TP = true_pose[i, :, :].squeeze()\n",
    "        PP = pred_pose[i, :, :].squeeze()\n",
    "        target = torch.tensor(pt_cld) @ TP[0:3, 0:3] + torch.cat(\n",
    "            (TP[0, 3].view(-1, 1), TP[1, 3].view(-1, 1), TP[2, 3].view(-1, 1)), 1)\n",
    "        output = torch.tensor(pt_cld) @ PP[0:3, 0:3] + torch.cat(\n",
    "            (PP[0, 3].view(-1, 1), PP[1, 3].view(-1, 1), PP[2, 3].view(-1, 1)), 1)\n",
    "        loss = (torch.abs(output - target).sum())/3000\n",
    "        if loss < 100:\n",
    "            total_loss = total_loss + loss\n",
    "        else:  # so that loss isn't NaN\n",
    "            total_loss = total_loss + torch.tensor([100.])\n",
    "\n",
    "    return total_loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set exists\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"LineMOD_Dataset\")\n",
    "    file1 = open('dataset_install.txt', 'r') \n",
    "    Lines = file1.readlines()\n",
    "    for url in Lines[:-1]:\n",
    "        zipresp = urlopen(url)\n",
    "        tempzip = open(\"tempfile.zip\", \"wb\")\n",
    "        tempzip.write(zipresp.read())\n",
    "        tempzip.close()\n",
    "\n",
    "        zf = ZipFile(\"tempfile.zip\")\n",
    "        zf.extractall(path = 'LineMOD_Dataset')\n",
    "        zf.close()\n",
    "        \n",
    "    zipresp = urlopen(Lines[-1])\n",
    "    tempzip = open(\"tempfile.zip\", \"wb\")\n",
    "    tempzip.write(zipresp.read())\n",
    "    tempzip.close()\n",
    "\n",
    "    zf = ZipFile(\"tempfile.zip\")\n",
    "    zf.extractall()\n",
    "    zf.close()\n",
    "except FileExistsError:\n",
    "    print(\"Data set exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Script to create the Ground Truth masks')\n",
    "parser.add_argument(\"--root_dir\", default=\"LineMOD_Dataset/\",\n",
    "                    help=\"path to dataset directory\")\n",
    "\n",
    "parser.add_argument(\"--bgd_dir\", default=\"val2017/\",\n",
    "                    help=\"path to background images dataset directory\")\n",
    "parser.add_argument(\"--split\", default=0.20, help=\"train:test split ratio\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 3160\n",
      "Testing Samples: 12641\n"
     ]
    }
   ],
   "source": [
    "root_dir = args.root_dir\n",
    "background_dir = args.bgd_dir\n",
    "\n",
    "list_all_images = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):  # images that exist\n",
    "            list_all_images.append(os.path.join(root, file))\n",
    "\n",
    "num_images = len(list_all_images)\n",
    "indices = list(range(num_images))\n",
    "np.random.seed(69)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(args.split * num_images))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "print(\"Training Samples:\", len(train_idx))\n",
    "print(\"Testing Samples:\", len(test_idx))\n",
    "\n",
    "save_obj(list_all_images, root_dir + \"all_images_adr\")\n",
    "save_obj(train_idx, root_dir + \"train_images_indices\")\n",
    "save_obj(test_idx, root_dir + \"test_images_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exist\n"
     ]
    }
   ],
   "source": [
    "classes = {'ape': 1, \n",
    "           'benchviseblue': 2, \n",
    "           'cam': 3, \n",
    "           'can': 4, \n",
    "           'cat': 5, \n",
    "           'driller': 6,\n",
    "           'duck': 7, \n",
    "           'eggbox': 8, \n",
    "           'glue': 9, \n",
    "           'holepuncher': 10, \n",
    "           'iron': 11, \n",
    "           'lamp': 12, \n",
    "           'phone': 13}\n",
    "class_names = list(classes.keys())\n",
    "dataset_dir_structure(root_dir, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 572.41140\n",
    "px = 325.26110\n",
    "fy = 573.57043\n",
    "py = 242.04899\n",
    "intrinsic_matrix = np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Start creating ground truth ------\n",
      "0/3160 finished!\n",
      "1000/3160 finished!\n",
      "2000/3160 finished!\n",
      "3000/3160 finished!\n",
      "----- Finished creating ground truth -----\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Start creating ground truth ------\")\n",
    "create_GT_masks(root_dir, background_dir, intrinsic_matrix, classes)\n",
    "create_UV_XYZ_dictionary(root_dir)  # create UV - XYZ dictionaries\n",
    "print(\"----- Finished creating ground truth -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Started training of the correspondence block ------\n",
      "------ Epoch  1  ---------\n",
      "Epoch: 1 \tTraining Loss: 0.232343 \tValidation Loss: 0.101496\n",
      "Validation loss decreased (inf --> 0.101496).  Saving model ...\n",
      "------ Epoch  2  ---------\n",
      "Epoch: 2 \tTraining Loss: 0.090479 \tValidation Loss: 0.080263\n",
      "Validation loss decreased (0.101496 --> 0.080263).  Saving model ...\n",
      "------ Epoch  3  ---------\n",
      "Epoch: 3 \tTraining Loss: 0.076805 \tValidation Loss: 0.068050\n",
      "Validation loss decreased (0.080263 --> 0.068050).  Saving model ...\n",
      "------ Epoch  4  ---------\n",
      "Epoch: 4 \tTraining Loss: 0.069335 \tValidation Loss: 0.063505\n",
      "Validation loss decreased (0.068050 --> 0.063505).  Saving model ...\n",
      "------ Epoch  5  ---------\n",
      "Epoch: 5 \tTraining Loss: 0.064060 \tValidation Loss: 0.062225\n",
      "Validation loss decreased (0.063505 --> 0.062225).  Saving model ...\n",
      "------ Epoch  6  ---------\n",
      "Epoch: 6 \tTraining Loss: 0.061415 \tValidation Loss: 0.059356\n",
      "Validation loss decreased (0.062225 --> 0.059356).  Saving model ...\n",
      "------ Epoch  7  ---------\n",
      "Epoch: 7 \tTraining Loss: 0.058685 \tValidation Loss: 0.058763\n",
      "Validation loss decreased (0.059356 --> 0.058763).  Saving model ...\n",
      "------ Epoch  8  ---------\n",
      "Epoch: 8 \tTraining Loss: 0.057080 \tValidation Loss: 0.055079\n",
      "Validation loss decreased (0.058763 --> 0.055079).  Saving model ...\n",
      "------ Epoch  9  ---------\n",
      "Epoch: 9 \tTraining Loss: 0.056084 \tValidation Loss: 0.055527\n",
      "------ Epoch  10  ---------\n",
      "Epoch: 10 \tTraining Loss: 0.054246 \tValidation Loss: 0.058265\n",
      "------ Epoch  11  ---------\n",
      "Epoch: 11 \tTraining Loss: 0.054495 \tValidation Loss: 0.051624\n",
      "Validation loss decreased (0.055079 --> 0.051624).  Saving model ...\n",
      "------ Epoch  12  ---------\n",
      "Epoch: 12 \tTraining Loss: 0.053172 \tValidation Loss: 0.053160\n",
      "------ Epoch  13  ---------\n",
      "Epoch: 13 \tTraining Loss: 0.051200 \tValidation Loss: 0.051459\n",
      "Validation loss decreased (0.051624 --> 0.051459).  Saving model ...\n",
      "------ Epoch  14  ---------\n",
      "Epoch: 14 \tTraining Loss: 0.051058 \tValidation Loss: 0.050407\n",
      "Validation loss decreased (0.051459 --> 0.050407).  Saving model ...\n",
      "------ Epoch  15  ---------\n",
      "Epoch: 15 \tTraining Loss: 0.049815 \tValidation Loss: 0.051344\n",
      "------ Epoch  16  ---------\n",
      "Epoch: 16 \tTraining Loss: 0.050008 \tValidation Loss: 0.053508\n",
      "------ Epoch  17  ---------\n",
      "Epoch: 17 \tTraining Loss: 0.048046 \tValidation Loss: 0.048560\n",
      "Validation loss decreased (0.050407 --> 0.048560).  Saving model ...\n",
      "------ Epoch  18  ---------\n",
      "Epoch: 18 \tTraining Loss: 0.047892 \tValidation Loss: 0.057663\n",
      "------ Epoch  19  ---------\n",
      "Epoch: 19 \tTraining Loss: 0.047999 \tValidation Loss: 0.046930\n",
      "Validation loss decreased (0.048560 --> 0.046930).  Saving model ...\n",
      "------ Epoch  20  ---------\n",
      "Epoch: 20 \tTraining Loss: 0.045181 \tValidation Loss: 0.046261\n",
      "Validation loss decreased (0.046930 --> 0.046261).  Saving model ...\n",
      "------ Epoch  21  ---------\n",
      "Epoch: 21 \tTraining Loss: 0.047485 \tValidation Loss: 0.054218\n",
      "------ Epoch  22  ---------\n",
      "Epoch: 22 \tTraining Loss: 0.046200 \tValidation Loss: 0.045089\n",
      "Validation loss decreased (0.046261 --> 0.045089).  Saving model ...\n",
      "------ Epoch  23  ---------\n",
      "Epoch: 23 \tTraining Loss: 0.043618 \tValidation Loss: 0.044447\n",
      "Validation loss decreased (0.045089 --> 0.044447).  Saving model ...\n",
      "------ Epoch  24  ---------\n",
      "Epoch: 24 \tTraining Loss: 0.046067 \tValidation Loss: 0.046369\n",
      "------ Epoch  25  ---------\n",
      "Epoch: 25 \tTraining Loss: 0.042968 \tValidation Loss: 0.043707\n",
      "Validation loss decreased (0.044447 --> 0.043707).  Saving model ...\n",
      "correspondance_train_loss_history [0.23234317620296643, 0.09047948930978397, 0.07680492890008454, 0.06933519622028063, 0.06406017827883928, 0.061415269181038006, 0.05868531807172525, 0.057079552737642314, 0.05608350842236246, 0.05424590710008258, 0.05449450041957294, 0.05317157164770119, 0.05120046388452189, 0.05105800927864223, 0.04981472868696327, 0.050008441875629785, 0.04804574146784276, 0.04789223349535295, 0.047999297044699706, 0.045180711874596866, 0.047485258235725795, 0.04619972115748008, 0.04361786284849425, 0.04606712582120318, 0.04296831949319266]\n",
      "correspondance_valid_loss_history [0.10149550895336308, 0.08026333334796791, 0.06804992739535586, 0.06350542494107651, 0.06222531015548525, 0.05935587321372726, 0.05876322324045851, 0.05507861715564622, 0.05552701017807556, 0.058265067636966705, 0.051624386512403246, 0.05316011651265848, 0.051458691058185284, 0.05040677493037302, 0.05134408724128823, 0.05350834110020837, 0.048559772334049775, 0.05766346678137779, 0.046930234926410866, 0.04626113027805769, 0.054218143578382984, 0.045089364499796794, 0.04444730635496635, 0.04636885656067465, 0.043706820664715165]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Training Finished -------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Started training of the correspondence block ------\")\n",
    "torch.cuda.empty_cache()\n",
    "train_correspondence_block(root_dir, classes, epochs=25, batch_size=5)\n",
    "print(\"-------------------- Training Finished -------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Started Initial pose estimation ------\n",
      "0/3160 finished!\n",
      "1000/3160 finished!\n",
      "2000/3160 finished!\n",
      "3000/3160 finished!\n",
      "Number of instances where PnP couldn't be used:  55\n",
      "------ Finished Initial pose estimation -----\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Started Initial pose estimation ------\")\n",
    "torch.cuda.empty_cache()\n",
    "initial_pose_estimation(root_dir, classes, intrinsic_matrix)\n",
    "print(\"------ Finished Initial pose estimation -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Started creating inputs for DL based pose refinement ------\n",
      "0/3160 finished!\n",
      "1000/3160 finished!\n",
      "2000/3160 finished!\n",
      "3000/3160 finished!\n",
      "Number of outliers:  6\n",
      "----- Finished creating inputs for DL based pose refinement -----\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Started creating inputs for DL based pose refinement ------\")\n",
    "create_refinement_inputs(root_dir, classes, intrinsic_matrix)\n",
    "print(\"----- Finished creating inputs for DL based pose refinement -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Started training DL based pose refiner ------\n",
      "----- Epoch Number:  1 --------\n",
      "Epoch: 1 \tTraining Loss: 41.385754 \tValidation Loss: 54.485176\n",
      "Validation loss decreased (inf --> 54.485176).  Saving model ...\n",
      "----- Epoch Number:  2 --------\n",
      "Epoch: 2 \tTraining Loss: 28.809266 \tValidation Loss: 46.750640\n",
      "Validation loss decreased (54.485176 --> 46.750640).  Saving model ...\n",
      "----- Epoch Number:  3 --------\n",
      "Epoch: 3 \tTraining Loss: 28.273817 \tValidation Loss: 56.537318\n",
      "----- Epoch Number:  4 --------\n",
      "Epoch: 4 \tTraining Loss: 28.360586 \tValidation Loss: 67.294473\n",
      "----- Epoch Number:  5 --------\n",
      "Epoch: 5 \tTraining Loss: 27.772457 \tValidation Loss: 51.597067\n",
      "----- Epoch Number:  6 --------\n",
      "Epoch: 6 \tTraining Loss: 27.479509 \tValidation Loss: 48.681757\n",
      "----- Epoch Number:  7 --------\n",
      "Epoch: 7 \tTraining Loss: 27.240248 \tValidation Loss: 54.405298\n",
      "----- Epoch Number:  8 --------\n",
      "Epoch: 8 \tTraining Loss: 27.423427 \tValidation Loss: 50.075631\n",
      "----- Epoch Number:  9 --------\n",
      "Epoch: 9 \tTraining Loss: 27.375251 \tValidation Loss: 57.142091\n",
      "----- Epoch Number:  10 --------\n",
      "Epoch: 10 \tTraining Loss: 27.723306 \tValidation Loss: 46.048288\n",
      "Validation loss decreased (46.750640 --> 46.048288).  Saving model ...\n",
      "----- Finished training DL based pose refiner ------\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Started training DL based pose refiner ------\")\n",
    "train_pose_refinement(root_dir, classes, epochs=10)\n",
    "print(\"----- Finished training DL based pose refiner ------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-8b79ee6198a2>:13: RuntimeWarning: overflow encountered in true_divide\n",
      "  coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Time: 0.9902797155380249\n",
      "ADD Score for all testing images is:  0.316\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c925e9c0402a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Evaluation Time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ADD Score for all testing images is: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mfloat_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfloat_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ADD float score for all testing images is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Script to create the Ground Truth masks')\n",
    "parser.add_argument(\"--root_dir\", default=\"LineMOD_Dataset/\",\n",
    "                    help=\"path to dataset directory\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "root_dir = args.root_dir\n",
    "\n",
    "classes = {'ape': 1, 'benchviseblue': 2, 'cam': 3, 'can': 4, 'cat': 5, 'driller': 6,\n",
    "           'duck': 7, 'eggbox': 8, 'glue': 9, 'holepuncher': 10, 'iron': 11, 'lamp': 12, 'phone': 13}\n",
    "\n",
    "score_card = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
    "              'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
    "\n",
    "instances = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
    "             'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(mode=None),\n",
    "                                transforms.Resize(size=(224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "fx = 572.41140\n",
    "px = 325.26110\n",
    "fy = 573.57043\n",
    "py = 242.04899  # Intrinsic Parameters of the Camera\n",
    "intrinsic_matrix = np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])\n",
    "\n",
    "correspondence_block = UNet(n_channels=3, out_channels_id=14,\n",
    "                                 out_channels_uv=256, bilinear=True)\n",
    "# load the best weights from the training loop\n",
    "correspondence_block.load_state_dict(torch.load(\n",
    "    'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "pose_refiner = Pose_Refiner()\n",
    "# load the best weights from the training loop\n",
    "pose_refiner.load_state_dict(torch.load(\n",
    "    'pose_refiner.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "correspondence_block.cuda()\n",
    "pose_refiner.cuda()\n",
    "pose_refiner.eval()\n",
    "correspondence_block.eval()\n",
    "\n",
    "list_all_images = load_obj(root_dir + \"all_images_adr\")\n",
    "testing_images_idx = load_obj(root_dir + \"test_images_indices\")\n",
    "\n",
    "regex = re.compile(r'\\d+')\n",
    "upsampled = nn.Upsample(size=[240, 320], mode='bilinear', align_corners=False)\n",
    "total_score = 0\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"Evaluated_Images\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "start = time.time()\n",
    "\n",
    "num_samples = 500\n",
    "\n",
    "for i in range(500):\n",
    "\n",
    "    img_adr = list_all_images[testing_images_idx[i]]\n",
    "    label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "    idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "\n",
    "    tra_adr = root_dir + label + \"/data/tra\" + str(idx) + \".tra\"\n",
    "    rot_adr = root_dir + label + \"/data/rot\" + str(idx) + \".rot\"\n",
    "    true_pose = get_rot_tra(rot_adr, tra_adr)\n",
    "\n",
    "    test_img = cv2.imread(img_adr)\n",
    "    img_for_bounding_box = copy.deepcopy(test_img)\n",
    "    test_img = cv2.resize(\n",
    "        test_img, (test_img.shape[1]//2, test_img.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    test_img = torch.from_numpy(test_img).type(torch.double)\n",
    "    test_img = test_img.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "    if len(test_img.shape) != 4:\n",
    "        test_img = test_img.view(\n",
    "            1, test_img.shape[0], test_img.shape[1], test_img.shape[2])\n",
    "\n",
    "    # pass through correspondence block\n",
    "    idmask_pred, umask_pred, vmask_pred = correspondence_block(\n",
    "        test_img.float().cuda())\n",
    "\n",
    "    # convert the masks to 240,320 shape\n",
    "    temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "    upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
    "    vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
    "    coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
    "    if coord_2d[0].nelement() != 0:  # label is detected in the image\n",
    "        coord_2d = torch.cat((coord_2d[0].view(\n",
    "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
    "        dct_keys = tuple(dct_keys.numpy())\n",
    "        dct = load_obj(root_dir + label + \"/UV-XYZ_mapping\")\n",
    "        mapping_2d = []\n",
    "        mapping_3d = []\n",
    "        for count, (u, v) in enumerate(dct_keys):\n",
    "            if (u, v) in dct:\n",
    "                mapping_2d.append(np.array(coord_2d[count]))\n",
    "                mapping_3d.append(dct[(u, v)])\n",
    "\n",
    "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
    "        if len(mapping_2d) >= 6 or len(mapping_3d) >= 6:\n",
    "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
    "                                                          np.array(mapping_2d, dtype=np.float32), intrinsic_matrix, distCoeffs=None,\n",
    "                                                          iterationsCount=150, reprojectionError=1.0, flags=cv2.SOLVEPNP_P3P)\n",
    "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
    "            pred_pose = np.append(rot, tvecs, axis=1)\n",
    "\n",
    "        else:  # save an empty file\n",
    "            pred_pose = np.zeros((3, 4))\n",
    "\n",
    "        min_x = coord_2d[:, 0].min()\n",
    "        max_x = coord_2d[:, 0].max()\n",
    "        min_y = coord_2d[:, 1].min()\n",
    "        max_y = coord_2d[:, 1].max()\n",
    "        img = test_img.squeeze().transpose(1, 2).transpose(0, 2)\n",
    "        obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
    "        # saving in the correct format using upsampling\n",
    "        obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
    "        obj_img = upsampled(obj_img)\n",
    "        obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "        obj_img = transform(torch.as_tensor(obj_img, dtype=torch.float32))\n",
    "        # create rendering for an object\n",
    "        cropped_rendered_img = create_rendering_eval(\n",
    "            root_dir, intrinsic_matrix, label, pred_pose)\n",
    "        rendered_img = torch.from_numpy(cropped_rendered_img)\n",
    "        rendered_img = rendered_img.unsqueeze(dim=0)\n",
    "        rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
    "        rendered_img = upsampled(rendered_img)\n",
    "        rendered_img = rendered_img.squeeze()\n",
    "        rendered_img = transform(torch.as_tensor(\n",
    "            rendered_img, dtype=torch.float32))\n",
    "\n",
    "        if len(rendered_img.shape) != 4:\n",
    "            rendered_img = rendered_img.view(\n",
    "                1, rendered_img.shape[0], rendered_img.shape[1], rendered_img.shape[2])\n",
    "\n",
    "        if len(obj_img.shape) != 4:\n",
    "            obj_img = obj_img.view(\n",
    "                1, obj_img.shape[0], obj_img.shape[1],  obj_img.shape[2])\n",
    "        pred_pose = (torch.from_numpy(pred_pose)).unsqueeze(0)\n",
    "\n",
    "        # pose refinement to get final output\n",
    "        xy, z, rot = pose_refiner(obj_img.float().cuda(),\n",
    "                                  rendered_img.float().cuda(), pred_pose)\n",
    "        # below 2 lines are for outliers only - edge case                          \n",
    "        rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "        rot[rot == float(\"Inf\")] = 1\n",
    "        xy[torch.isnan(xy)] = 0\n",
    "        z[torch.isnan(z)] = 0\n",
    "        # convert R quarternion to rotational matrix\n",
    "        rot = (R.from_quat(rot.detach().cpu().numpy())).as_matrix()\n",
    "        pred_pose = pred_pose.squeeze().numpy()\n",
    "        # update predicted pose\n",
    "        xy = xy.squeeze()\n",
    "        pred_pose[0:3, 0:3] = rot\n",
    "        pred_pose[0, 3] = xy[0]\n",
    "        pred_pose[1, 3] = xy[1]\n",
    "        pred_pose[2, 3] = z\n",
    "\n",
    "        diameter = np.loadtxt(root_dir + label + \"/distance.txt\")\n",
    "        ptcld_file = root_dir + label + \"/object.xyz\"\n",
    "        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
    "        test_rendered = create_bounding_box(img_for_bounding_box, pred_pose, pt_cld, intrinsic_matrix)\n",
    "        test_rendered = create_bounding_box(test_rendered, true_pose, pt_cld, intrinsic_matrix, color = (0,255,0))\n",
    "\n",
    "        score = ADD_score(pt_cld, true_pose, pred_pose, diameter)\n",
    "        float_score = ADD_score_norm(pt_cld, true_pose, pred_pose, diameter)\n",
    "        path = \"Evaluated_Images/\" + str(float_score) + \"_score_\" + str(i) +'.png'\n",
    "        cv2.imwrite(path, test_rendered)\n",
    "        total_score += score\n",
    "        score_card[label] += score\n",
    "\n",
    "    else:\n",
    "        score_card[label] += 0\n",
    "\n",
    "    instances[label] += 1\n",
    "\n",
    "print(\"Average Evaluation Time:\", (time.time() - start)/num_samples)\n",
    "print(\"ADD Score for all testing images is: \",total_score/num_samples)\n",
    "float_score = [1 - score for score in float_score]\n",
    "print(\"ADD float score for all testing images is:\", sum(float_score)/num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1_id): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2_id): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3_id): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4_id): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc_id): OutConv(\n",
       "    (conv): Conv2d(64, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (up1_u): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2_u): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc_u1): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_u2): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_u3): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_u4): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (up3_u): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (up4_u): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (up1_v): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2_v): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc_v1): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_v2): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_v3): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (outc_v4): OutConv(\n",
       "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (up3_v): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "  (up4_v): Upsample(scale_factor=2.0, mode=bilinear)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "correspondence_block.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (inc): DoubleConv(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up1_id): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2_id): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3_id): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4_id): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc_id): OutConv(\n",
      "    (conv): Conv2d(64, 14, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (up1_u): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2_u): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc_u1): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_u2): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_u3): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_u4): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (up3_u): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (up4_u): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (up1_v): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2_v): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc_v1): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_v2): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_v3): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (outc_v4): OutConv(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (up3_v): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (up4_v): Upsample(scale_factor=2.0, mode=bilinear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(correspondence_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "transforms = [\n",
    "    # Fold Conv, BN, RELU layers into one\n",
    "    hl.transforms.Fold(\"Conv > BatchNorm > Relu\", \"ConvBnRelu\"),\n",
    "    # Fold Conv, BN layers together\n",
    "    hl.transforms.Fold(\"Conv > BatchNorm\", \"ConvBn\"),\n",
    "    # Fold bottleneck blocks\n",
    "    hl.transforms.Fold(\"\"\"\n",
    "        ((ConvBnRelu > ConvBnRelu > ConvBn) | ConvBn) > Add > Relu\n",
    "        \"\"\", \"BottleneckBlock\", \"Bottleneck Block\"),\n",
    "    # Fold residual blocks\n",
    "    hl.transforms.Fold(\"\"\"ConvBnRelu > ConvBnRelu > ConvBn > Add > Relu\"\"\",\n",
    "                       \"ResBlock\", \"Residual Block\"),\n",
    "    hl.transforms.Fold(\"\"\"ConvBnRelu > ConvBnRelu > ConvBn > Add > Relu\"\"\",\n",
    "                       \"ResBlock\", \"Residual Block\"),\n",
    "    # Fold repeated blocks\n",
    "    hl.transforms.FoldDuplicates(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-88b11db697d2>:58: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
      "<ipython-input-10-88b11db697d2>:58: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
      "<ipython-input-10-88b11db697d2>:59: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
      "<ipython-input-10-88b11db697d2>:59: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
      "/home/ecbm4040/anaconda3/envs/envTorch/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:242: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator. \n",
      "  warnings.warn(\"You are trying to export the model with \" + onnx_op + \" for ONNX opset version \"\n",
      "/home/ecbm4040/anaconda3/envs/envTorch/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:176: UserWarning: ONNX export failed on upsample_bilinear2d because align_corners == True not supported\n",
      "  warnings.warn(\"ONNX export failed on \" + op + \" because \" + msg + \" not supported\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "hl.build_graph(correspondence_block, torch.zeros([1, 3, 640, 480]).cuda(), transforms).save(\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 640, 480]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 640, 480]             128\n",
      "              ReLU-3         [-1, 64, 640, 480]               0\n",
      "            Conv2d-4         [-1, 64, 640, 480]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 640, 480]             128\n",
      "              ReLU-6         [-1, 64, 640, 480]               0\n",
      "        DoubleConv-7         [-1, 64, 640, 480]               0\n",
      "         MaxPool2d-8         [-1, 64, 320, 240]               0\n",
      "            Conv2d-9        [-1, 128, 320, 240]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 320, 240]             256\n",
      "             ReLU-11        [-1, 128, 320, 240]               0\n",
      "           Conv2d-12        [-1, 128, 320, 240]         147,584\n",
      "      BatchNorm2d-13        [-1, 128, 320, 240]             256\n",
      "             ReLU-14        [-1, 128, 320, 240]               0\n",
      "       DoubleConv-15        [-1, 128, 320, 240]               0\n",
      "             Down-16        [-1, 128, 320, 240]               0\n",
      "        MaxPool2d-17        [-1, 128, 160, 120]               0\n",
      "           Conv2d-18        [-1, 256, 160, 120]         295,168\n",
      "      BatchNorm2d-19        [-1, 256, 160, 120]             512\n",
      "             ReLU-20        [-1, 256, 160, 120]               0\n",
      "           Conv2d-21        [-1, 256, 160, 120]         590,080\n",
      "      BatchNorm2d-22        [-1, 256, 160, 120]             512\n",
      "             ReLU-23        [-1, 256, 160, 120]               0\n",
      "       DoubleConv-24        [-1, 256, 160, 120]               0\n",
      "             Down-25        [-1, 256, 160, 120]               0\n",
      "        MaxPool2d-26          [-1, 256, 80, 60]               0\n",
      "           Conv2d-27          [-1, 512, 80, 60]       1,180,160\n",
      "      BatchNorm2d-28          [-1, 512, 80, 60]           1,024\n",
      "             ReLU-29          [-1, 512, 80, 60]               0\n",
      "           Conv2d-30          [-1, 512, 80, 60]       2,359,808\n",
      "      BatchNorm2d-31          [-1, 512, 80, 60]           1,024\n",
      "             ReLU-32          [-1, 512, 80, 60]               0\n",
      "       DoubleConv-33          [-1, 512, 80, 60]               0\n",
      "             Down-34          [-1, 512, 80, 60]               0\n",
      "        MaxPool2d-35          [-1, 512, 40, 30]               0\n",
      "           Conv2d-36          [-1, 512, 40, 30]       2,359,808\n",
      "      BatchNorm2d-37          [-1, 512, 40, 30]           1,024\n",
      "             ReLU-38          [-1, 512, 40, 30]               0\n",
      "           Conv2d-39          [-1, 512, 40, 30]       2,359,808\n",
      "      BatchNorm2d-40          [-1, 512, 40, 30]           1,024\n",
      "             ReLU-41          [-1, 512, 40, 30]               0\n",
      "       DoubleConv-42          [-1, 512, 40, 30]               0\n",
      "             Down-43          [-1, 512, 40, 30]               0\n",
      "         Upsample-44          [-1, 512, 80, 60]               0\n",
      "           Conv2d-45          [-1, 512, 80, 60]       4,719,104\n",
      "      BatchNorm2d-46          [-1, 512, 80, 60]           1,024\n",
      "             ReLU-47          [-1, 512, 80, 60]               0\n",
      "           Conv2d-48          [-1, 256, 80, 60]       1,179,904\n",
      "      BatchNorm2d-49          [-1, 256, 80, 60]             512\n",
      "             ReLU-50          [-1, 256, 80, 60]               0\n",
      "       DoubleConv-51          [-1, 256, 80, 60]               0\n",
      "               Up-52          [-1, 256, 80, 60]               0\n",
      "         Upsample-53        [-1, 256, 160, 120]               0\n",
      "           Conv2d-54        [-1, 256, 160, 120]       1,179,904\n",
      "      BatchNorm2d-55        [-1, 256, 160, 120]             512\n",
      "             ReLU-56        [-1, 256, 160, 120]               0\n",
      "           Conv2d-57        [-1, 128, 160, 120]         295,040\n",
      "      BatchNorm2d-58        [-1, 128, 160, 120]             256\n",
      "             ReLU-59        [-1, 128, 160, 120]               0\n",
      "       DoubleConv-60        [-1, 128, 160, 120]               0\n",
      "               Up-61        [-1, 128, 160, 120]               0\n",
      "         Upsample-62        [-1, 128, 320, 240]               0\n",
      "           Conv2d-63        [-1, 128, 320, 240]         295,040\n",
      "      BatchNorm2d-64        [-1, 128, 320, 240]             256\n",
      "             ReLU-65        [-1, 128, 320, 240]               0\n",
      "           Conv2d-66         [-1, 64, 320, 240]          73,792\n",
      "      BatchNorm2d-67         [-1, 64, 320, 240]             128\n",
      "             ReLU-68         [-1, 64, 320, 240]               0\n",
      "       DoubleConv-69         [-1, 64, 320, 240]               0\n",
      "               Up-70         [-1, 64, 320, 240]               0\n",
      "         Upsample-71         [-1, 64, 640, 480]               0\n",
      "           Conv2d-72         [-1, 64, 640, 480]          73,792\n",
      "      BatchNorm2d-73         [-1, 64, 640, 480]             128\n",
      "             ReLU-74         [-1, 64, 640, 480]               0\n",
      "           Conv2d-75         [-1, 64, 640, 480]          36,928\n",
      "      BatchNorm2d-76         [-1, 64, 640, 480]             128\n",
      "             ReLU-77         [-1, 64, 640, 480]               0\n",
      "       DoubleConv-78         [-1, 64, 640, 480]               0\n",
      "               Up-79         [-1, 64, 640, 480]               0\n",
      "           Conv2d-80         [-1, 14, 640, 480]             910\n",
      "          OutConv-81         [-1, 14, 640, 480]               0\n",
      "         Upsample-82          [-1, 512, 80, 60]               0\n",
      "           Conv2d-83          [-1, 512, 80, 60]       4,719,104\n",
      "      BatchNorm2d-84          [-1, 512, 80, 60]           1,024\n",
      "             ReLU-85          [-1, 512, 80, 60]               0\n",
      "           Conv2d-86          [-1, 256, 80, 60]       1,179,904\n",
      "      BatchNorm2d-87          [-1, 256, 80, 60]             512\n",
      "             ReLU-88          [-1, 256, 80, 60]               0\n",
      "       DoubleConv-89          [-1, 256, 80, 60]               0\n",
      "               Up-90          [-1, 256, 80, 60]               0\n",
      "         Upsample-91        [-1, 256, 160, 120]               0\n",
      "           Conv2d-92        [-1, 256, 160, 120]       1,179,904\n",
      "      BatchNorm2d-93        [-1, 256, 160, 120]             512\n",
      "             ReLU-94        [-1, 256, 160, 120]               0\n",
      "           Conv2d-95        [-1, 256, 160, 120]         590,080\n",
      "      BatchNorm2d-96        [-1, 256, 160, 120]             512\n",
      "             ReLU-97        [-1, 256, 160, 120]               0\n",
      "       DoubleConv-98        [-1, 256, 160, 120]               0\n",
      "               Up-99        [-1, 256, 160, 120]               0\n",
      "          Conv2d-100        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-101        [-1, 256, 160, 120]               0\n",
      "          Conv2d-102        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-103        [-1, 256, 160, 120]               0\n",
      "          Conv2d-104        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-105        [-1, 256, 160, 120]               0\n",
      "        Upsample-106        [-1, 256, 320, 240]               0\n",
      "        Upsample-107        [-1, 256, 640, 480]               0\n",
      "          Conv2d-108        [-1, 256, 640, 480]          65,792\n",
      "         OutConv-109        [-1, 256, 640, 480]               0\n",
      "        Upsample-110          [-1, 512, 80, 60]               0\n",
      "          Conv2d-111          [-1, 512, 80, 60]       4,719,104\n",
      "     BatchNorm2d-112          [-1, 512, 80, 60]           1,024\n",
      "            ReLU-113          [-1, 512, 80, 60]               0\n",
      "          Conv2d-114          [-1, 256, 80, 60]       1,179,904\n",
      "     BatchNorm2d-115          [-1, 256, 80, 60]             512\n",
      "            ReLU-116          [-1, 256, 80, 60]               0\n",
      "      DoubleConv-117          [-1, 256, 80, 60]               0\n",
      "              Up-118          [-1, 256, 80, 60]               0\n",
      "        Upsample-119        [-1, 256, 160, 120]               0\n",
      "          Conv2d-120        [-1, 256, 160, 120]       1,179,904\n",
      "     BatchNorm2d-121        [-1, 256, 160, 120]             512\n",
      "            ReLU-122        [-1, 256, 160, 120]               0\n",
      "          Conv2d-123        [-1, 256, 160, 120]         590,080\n",
      "     BatchNorm2d-124        [-1, 256, 160, 120]             512\n",
      "            ReLU-125        [-1, 256, 160, 120]               0\n",
      "      DoubleConv-126        [-1, 256, 160, 120]               0\n",
      "              Up-127        [-1, 256, 160, 120]               0\n",
      "          Conv2d-128        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-129        [-1, 256, 160, 120]               0\n",
      "          Conv2d-130        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-131        [-1, 256, 160, 120]               0\n",
      "          Conv2d-132        [-1, 256, 160, 120]          65,792\n",
      "         OutConv-133        [-1, 256, 160, 120]               0\n",
      "        Upsample-134        [-1, 256, 320, 240]               0\n",
      "        Upsample-135        [-1, 256, 640, 480]               0\n",
      "          Conv2d-136        [-1, 256, 640, 480]          65,792\n",
      "         OutConv-137        [-1, 256, 640, 480]               0\n",
      "================================================================\n",
      "Total params: 33,137,678\n",
      "Trainable params: 33,137,678\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.52\n",
      "Forward/backward pass size (MB): 9745.31\n",
      "Params size (MB): 126.41\n",
      "Estimated Total Size (MB): 9875.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(correspondence_block, (3, 640, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
