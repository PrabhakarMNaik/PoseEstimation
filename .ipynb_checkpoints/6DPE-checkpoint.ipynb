{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "import re\n",
    "import cv2\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import models\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    \"\"\"\n",
    "    Create dictionaries for the training and testing data\n",
    "    args: 1. Object to be stored\n",
    "          2. Identifier for the object\n",
    "    \"\"\"\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    \"\"\"\n",
    "    Loads a pickle object\n",
    "    args: name: name of the object\n",
    "    returns: object stored in the file\n",
    "    \"\"\"\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def ADD_score(pt_cld, true_pose, pred_pose, diameter):\n",
    "    \"Evaluation metric - ADD score\"\n",
    "    pred_pose[0:3, 0:3][np.isnan(pred_pose[0:3, 0:3])] = 1\n",
    "    pred_pose[:, 3][np.isnan(pred_pose[:, 3])] = 0\n",
    "    target = pt_cld @ true_pose[0:3, 0:3] + np.array(\n",
    "        [true_pose[0, 3], true_pose[1, 3], true_pose[2, 3]])\n",
    "    output = pt_cld @ pred_pose[0:3, 0:3] + np.array(\n",
    "        [pred_pose[0, 3], pred_pose[1, 3], pred_pose[2, 3]])\n",
    "    avg_distance = (np.linalg.norm(output - target))/pt_cld.shape[0]\n",
    "    threshold = diameter * 0.1\n",
    "    if avg_distance <= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_dir_structure(root_dir, class_names):\n",
    "    \"\"\"\n",
    "    Creates a directory structure for the classes of the objects\n",
    "    present in the dataset.\n",
    "    args: 1. root_dir: parent directory\n",
    "          2. class_names: classes of samples in the dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for label in class_names:  # create directories to store data\n",
    "            os.mkdir(root_dir + label + \"/predicted_pose\")\n",
    "            os.mkdir(root_dir + label + \"/ground_truth\")\n",
    "            os.mkdir(root_dir + label + \"/ground_truth/IDmasks\")\n",
    "            os.mkdir(root_dir + label + \"/ground_truth/Umasks\")\n",
    "            os.mkdir(root_dir + label + \"/ground_truth/Vmasks\")\n",
    "            os.mkdir(root_dir + label + \"/changed_background\")\n",
    "            os.mkdir(root_dir + label + \"/pose_refinement\")\n",
    "            os.mkdir(root_dir + label + \"/pose_refinement/real\")\n",
    "            os.mkdir(root_dir + label + \"/pose_refinement/rendered\")\n",
    "    except FileExistsError:\n",
    "        print(\"Directories already exist\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_tra(rot_adr, tra_adr):\n",
    "    \"\"\"\n",
    "    Helper function to the read the rotation and translation file\n",
    "        Args:\n",
    "                rot_adr (str): path to the file containing rotation of an object\n",
    "        tra_adr (str): path to the file containing translation of an object\n",
    "        Returns:\n",
    "                rigid transformation (np array): rotation and translation matrix combined\n",
    "    \"\"\"\n",
    "\n",
    "    rot_matrix = np.loadtxt(rot_adr, skiprows=1)\n",
    "    trans_matrix = np.loadtxt(tra_adr, skiprows=1)\n",
    "    trans_matrix = np.reshape(trans_matrix, (3, 1))\n",
    "    rigid_transformation = np.append(rot_matrix, trans_matrix, axis=1)\n",
    "\n",
    "    return rigid_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_holes(idmask, umask, vmask):\n",
    "    \"\"\"\n",
    "    Helper function to fill the holes in id , u and vmasks\n",
    "        Args:\n",
    "                idmask (np.array): id mask whose holes you want to fill\n",
    "        umask (np.array): u mask whose holes you want to fill\n",
    "        vmask (np.array): v mask whose holes you want to fill\n",
    "        Returns:\n",
    "                filled_id_mask (np array): id mask with holes filled\n",
    "        filled_u_mask (np array): u mask with holes filled\n",
    "        filled_id_mask (np array): v mask with holes filled\n",
    "    \"\"\"\n",
    "    idmask = np.array(idmask, dtype='float32')\n",
    "    umask = np.array(umask, dtype='float32')\n",
    "    vmask = np.array(vmask, dtype='float32')\n",
    "    thr, im_th = cv2.threshold(idmask, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    res = cv2.morphologyEx(im_th, cv2.MORPH_OPEN, kernel)\n",
    "    im_th = cv2.bitwise_not(im_th)\n",
    "    des = cv2.bitwise_not(res)\n",
    "    mask = np.array(des-im_th, dtype='uint8')\n",
    "    filled_id_mask = cv2.inpaint(idmask, mask, 5, cv2.INPAINT_TELEA)\n",
    "    filled_u_mask = cv2.inpaint(umask, mask, 5, cv2.INPAINT_TELEA)\n",
    "    filled_v_mask = cv2.inpaint(vmask, mask, 5, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return filled_id_mask, filled_u_mask, filled_v_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_GT_masks(root_dir, background_dir, intrinsic_matrix,classes):\n",
    "    \"\"\"\n",
    "    Helper function to create the Ground Truth ID,U and V masks\n",
    "        Args:\n",
    "        root_dir (str): path to the root directory of the dataset\n",
    "        background_dir(str): path t\n",
    "        intrinsic_matrix (array): matrix containing camera intrinsics\n",
    "        classes (dict) : dictionary containing classes and their ids\n",
    "        Saves the masks to their respective directories\n",
    "    \"\"\"\n",
    "    list_all_images = load_obj(root_dir + \"all_images_adr\")\n",
    "    training_images_idx = load_obj(root_dir + \"train_images_indices\")\n",
    "    for i in range(len(training_images_idx)):\n",
    "        img_adr = list_all_images[training_images_idx[i]]\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        regex = re.compile(r'\\d+')\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \"/\" + str(len(training_images_idx)) + \" finished!\")\n",
    "\n",
    "        image = cv2.imread(img_adr)\n",
    "        ID_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "        U_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "        V_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "\n",
    "        ID_mask_file = root_dir + label + \\\n",
    "            \"/ground_truth/IDmasks/color\" + str(idx) + \".png\"\n",
    "        U_mask_file = root_dir + label + \\\n",
    "            \"/ground_truth/Umasks/color\" + str(idx) + \".png\"\n",
    "        V_mask_file = root_dir + label + \\\n",
    "            \"/ground_truth/Vmasks/color\" + str(idx) + \".png\"\n",
    "\n",
    "        tra_adr = root_dir + label + \"/data/tra\" + str(idx) + \".tra\"\n",
    "        rot_adr = root_dir + label + \"/data/rot\" + str(idx) + \".rot\"\n",
    "        rigid_transformation = get_rot_tra(rot_adr, tra_adr)\n",
    "\n",
    "        # Read point Point Cloud Data\n",
    "        ptcld_file = root_dir + label + \"/object.xyz\"\n",
    "        pt_cld_data = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
    "        ones = np.ones((pt_cld_data.shape[0], 1))\n",
    "        homogenous_coordinate = np.append(pt_cld_data[:, :3], ones, axis=1)\n",
    "\n",
    "        # Perspective Projection to obtain 2D coordinates for masks\n",
    "        homogenous_2D = intrinsic_matrix @ (\n",
    "            rigid_transformation @ homogenous_coordinate.T)\n",
    "        coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
    "        coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
    "        x_2d = np.clip(coord_2D[:, 0], 0, 639)\n",
    "        y_2d = np.clip(coord_2D[:, 1], 0, 479)\n",
    "        ID_mask[y_2d, x_2d] = classes[label]\n",
    "\n",
    "        if i % 100 != 0:  # change background for every 99/100 images\n",
    "            background_img_adr = background_dir + \\\n",
    "                random.choice(os.listdir(background_dir))\n",
    "            background_img = cv2.imread(background_img_adr)\n",
    "            background_img = cv2.resize(\n",
    "                background_img, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "            background_img[y_2d, x_2d, :] = image[y_2d, x_2d, :]\n",
    "            background_adr = root_dir + label + \\\n",
    "                \"/changed_background/color\" + str(idx) + \".png\"\n",
    "            mpimg.imsave(background_adr, background_img)\n",
    "\n",
    "        # Generate Ground Truth UV Maps\n",
    "        centre = np.mean(pt_cld_data, axis=0)\n",
    "        length = np.sqrt((centre[0]-pt_cld_data[:, 0])**2 + (centre[1] -\n",
    "                                                             pt_cld_data[:, 1])**2 + (centre[2]-pt_cld_data[:, 2])**2)\n",
    "        unit_vector = [(pt_cld_data[:, 0]-centre[0])/length, (pt_cld_data[:,\n",
    "                                                                          1]-centre[1])/length, (pt_cld_data[:, 2]-centre[2])/length]\n",
    "        U = 0.5 + (np.arctan2(unit_vector[2], unit_vector[0])/(2*np.pi))\n",
    "        V = 0.5 - (np.arcsin(unit_vector[1])/np.pi)\n",
    "        U_mask[y_2d, x_2d] = U\n",
    "        V_mask[y_2d, x_2d] = V\n",
    "\n",
    "        # Saving ID, U and V masks after using the fill holes function\n",
    "        ID_mask, U_mask, V_mask = fill_holes(ID_mask, U_mask, V_mask)\n",
    "        cv2.imwrite(ID_mask_file, ID_mask)\n",
    "        mpimg.imsave(U_mask_file, U_mask, cmap='gray')\n",
    "        mpimg.imsave(V_mask_file, V_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_UV_XYZ_dictionary(root_dir):\n",
    "\n",
    "    classes = ['ape', 'benchviseblue', 'can', 'cat', 'driller', 'duck', 'glue', 'holepuncher',\n",
    "               'iron', 'lamp', 'phone', 'cam', 'eggbox']\n",
    "    # create a dictionary for UV to XYZ correspondence\n",
    "    for label in classes:\n",
    "        ptcld_file = root_dir + label + \"/object.xyz\"\n",
    "        pt_cld_data = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
    "        # calculate u and v coordinates from the xyz point cloud file\n",
    "        centre = np.mean(pt_cld_data, axis=0)\n",
    "        length = np.sqrt((centre[0]-pt_cld_data[:, 0])**2 + (centre[1] -\n",
    "                                                             pt_cld_data[:, 1])**2 + (centre[2]-pt_cld_data[:, 2])**2)\n",
    "        unit_vector = [(pt_cld_data[:, 0]-centre[0])/length, (pt_cld_data[:,\n",
    "                                                                          1]-centre[1])/length, (pt_cld_data[:, 2]-centre[2])/length]\n",
    "        u_coord = 0.5 + (np.arctan2(unit_vector[2], unit_vector[0])/(2*np.pi))\n",
    "        v_coord = 0.5 - (np.arcsin(unit_vector[1])/np.pi)\n",
    "        u_coord = (u_coord * 255).astype(int)\n",
    "        v_coord = (v_coord * 255).astype(int)\n",
    "        # save the mapping as a pickle file\n",
    "        dct = {}\n",
    "        for u, v, xyz in zip(u_coord, v_coord, pt_cld_data):\n",
    "            key = (u, v)\n",
    "            if key not in dct:\n",
    "                dct[key] = xyz\n",
    "        save_obj(dct, root_dir + label + \"/UV-XYZ_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class LineMODDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir (str): path to the dataset\n",
    "        classes (dictionary): values of classes to extract from segmentation mask \n",
    "        transform : Transforms for input image\n",
    "            \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, classes=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n",
    "        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_images_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        img_adr = self.list_all_images[self.training_images_idx[i]]\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        regex = re.compile(r'\\d+')\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        \n",
    "        if i % 100 != 0:  # read the image with changed background\n",
    "            image = cv2.imread(self.root_dir + label +\n",
    "                               \"/changed_background/color\" + str(idx) + \".png\")\n",
    "        else:\n",
    "            image = cv2.imread(img_adr)\n",
    "\n",
    "        IDmask = cv2.imread(self.root_dir + label + \"/ground_truth/IDmasks/color\" +\n",
    "                            str(idx) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        Umask = cv2.imread(self.root_dir + label + \"/ground_truth/Umasks/color\" +\n",
    "                           str(idx) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        Vmask = cv2.imread(self.root_dir + label + \"/ground_truth/Vmasks/color\" +\n",
    "                           str(idx) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        # resize the masks\n",
    "        image = cv2.resize(\n",
    "            image, (image.shape[1]//2, image.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "        IDmask = cv2.resize(\n",
    "            IDmask, (IDmask.shape[1]//2, IDmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "        Umask = cv2.resize(\n",
    "            Umask, (Umask.shape[1]//2, Umask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "        Vmask = cv2.resize(\n",
    "            Vmask, (Vmask.shape[1]//2, Vmask.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        IDmask = (torch.from_numpy(IDmask)).type(torch.int64)\n",
    "        Umask = (torch.from_numpy(Umask)).type(torch.int64)\n",
    "        Vmask = (torch.from_numpy(Vmask)).type(torch.int64)\n",
    "        return img_adr, image, IDmask, Umask, Vmask\n",
    "\n",
    "\n",
    "class PoseRefinerDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir (str): path to the dataset directory\n",
    "        classes (dict): dictionary containing classes as key  \n",
    "        transform : Transforms for input image\n",
    "            \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, classes=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        self.list_all_images = load_obj(root_dir + \"all_images_adr\")\n",
    "        self.training_images_idx = load_obj(root_dir + \"train_images_indices\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_images_idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img_adr = self.list_all_images[self.training_images_idx[i]]\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        regex = re.compile(r'\\d+')\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        image = cv2.imread(self.root_dir + label +\n",
    "                           '/pose_refinement/real/color' + str(idx) + \".png\")\n",
    "        rendered = cv2.imread(\n",
    "            self.root_dir + label + '/pose_refinement/rendered/color' + str(idx) + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "        rendered = cv2.cvtColor(rendered.astype('uint8'), cv2.COLOR_GRAY2RGB)\n",
    "        true_pose = get_rot_tra(self.root_dir + label + '/data/rot' + str(idx) + \".rot\",\n",
    "                                self.root_dir + label + '/data/tra' + str(idx) + \".tra\")\n",
    "        pred_pose_adr = self.root_dir + label + \\\n",
    "            '/predicted_pose/info_' + str(idx) + \".txt\"\n",
    "        pred_pose = np.loadtxt(pred_pose_adr)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            rendered = self.transform(rendered)\n",
    "        return label, image, rendered, true_pose, pred_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_correspondence_block(root_dir, classes, epochs=20, batch_size = 3):\n",
    "\n",
    "    train_data = LineMODDataset(root_dir, classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)]))\n",
    "\n",
    "    \n",
    "    num_workers = 0\n",
    "    valid_size = 0.2\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    # architecture for correspondence block - 13 objects + backgound = 14 channels for ID masks\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "\n",
    "    # custom loss function and optimizer\n",
    "    criterion_id = nn.CrossEntropyLoss()\n",
    "    criterion_u = nn.CrossEntropyLoss()\n",
    "    criterion_v = nn.CrossEntropyLoss()\n",
    "\n",
    "    # specify optimizer\n",
    "    optimizer = optim.Adam(\n",
    "        correspondence_block.parameters(), lr=3e-4, weight_decay=3e-5)\n",
    "\n",
    "    # training loop\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = epochs\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        torch.cuda.empty_cache()\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        print(\"------ Epoch \", epoch, \" ---------\")\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        correspondence_block.train()\n",
    "        for _, image, idmask, umask, vmask in train_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            image, idmask, umask, vmask = image.cuda(\n",
    "            ), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
    "            # calculate the batch loss\n",
    "            loss_id = criterion_id(idmask_pred, idmask)\n",
    "            loss_u = criterion_u(umask_pred, umask)\n",
    "            loss_v = criterion_v(vmask_pred, vmask)\n",
    "            loss = loss_id + loss_u + loss_v\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        correspondence_block.eval()\n",
    "        for _, image, idmask, umask, vmask in valid_loader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            image, idmask, umask, vmask = image.cuda(\n",
    "            ), idmask.cuda(), umask.cuda(), vmask.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            idmask_pred, umask_pred, vmask_pred = correspondence_block(image)\n",
    "            # calculate the batch loss\n",
    "            loss_id = criterion_id(idmask_pred, idmask)\n",
    "            loss_u = criterion_u(umask_pred, umask)\n",
    "            loss_v = criterion_v(vmask_pred, vmask)\n",
    "            loss = loss_id + loss_u + loss_v\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,\n",
    "                valid_loss))\n",
    "            torch.save(correspondence_block.state_dict(),\n",
    "                       'correspondence_block.pt')\n",
    "            valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels // 2, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels = 3, out_channels_id = 9, out_channels_uv = 256, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.out_channels_id = out_channels_id\n",
    "        self.out_channels_uv = out_channels_uv\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024//factor)\n",
    "\n",
    "\n",
    "        #ID MASK\n",
    "        self.up1_id = Up(1024, 512, bilinear)\n",
    "        self.up2_id = Up(512, 256, bilinear)\n",
    "        self.up3_id = Up(256, 128, bilinear)\n",
    "        self.up4_id = Up(128, 64 * factor, bilinear)\n",
    "        self.outc_id = OutConv(64, out_channels_id)\n",
    "\n",
    "        #U Mask\n",
    "        self.up1_u = Up(1024, 512, bilinear)\n",
    "        self.up2_u = Up(512,512,bilinear)\n",
    "        self.outc_u1 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u2 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u3 = OutConv(256, out_channels_uv)\n",
    "        self.outc_u4 = OutConv(256, out_channels_uv)\n",
    "        self.up3_u = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4_u = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        #V Mask\n",
    "        self.up1_v = Up(1024, 512, bilinear)\n",
    "        self.up2_v = Up(512,512,bilinear)\n",
    "        self.outc_v1 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v2 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v3 = OutConv(256, out_channels_uv)\n",
    "        self.outc_v4 = OutConv(256, out_channels_uv)\n",
    "        self.up3_v = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4_v = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # ID mask\n",
    "        x_id = self.up1_id(x5, x4)\n",
    "        x_id = self.up2_id(x_id, x3)\n",
    "        x_id = self.up3_id(x_id, x2)\n",
    "        x_id = self.up4_id(x_id, x1)\n",
    "        logits_id = self.outc_id(x_id)\n",
    "\n",
    "        # U mask\n",
    "        x_u = self.up1_u(x5, x4)\n",
    "        x_u = self.up2_u(x_u,x3)\n",
    "        x_u = self.outc_u1(x_u)\n",
    "        x_u = self.outc_u2(x_u)\n",
    "        x_u = self.outc_u3(x_u)\n",
    "        x_u = self.up3_u(x_u)\n",
    "        x_u = self.up4_u(x_u)\n",
    "        logits_u = self.outc_u4(x_u)\n",
    "\n",
    "        # V mask\n",
    "        x_v = self.up1_v(x5, x4)\n",
    "        x_v = self.up2_v(x_v,x3)\n",
    "        x_v = self.outc_v1(x_v)\n",
    "        x_v = self.outc_v2(x_v)\n",
    "        x_v = self.outc_v3(x_v)\n",
    "        x_v = self.up3_v(x_v)\n",
    "        x_v = self.up4_v(x_v)\n",
    "        logits_v = self.outc_v4(x_v)\n",
    "        \n",
    "        return logits_id,logits_u, logits_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_pose_estimation(root_dir, classes, intrinsic_matrix):\n",
    "\n",
    "    # LineMOD Dataset\n",
    "    train_data = LineMODDataset(root_dir, classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    # load the best correspondence block weights\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "    correspondence_block.load_state_dict(torch.load(\n",
    "        'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "    # initial 6D pose prediction\n",
    "    regex = re.compile(r'\\d+')\n",
    "    outliers = 0\n",
    "    for i in range(len(train_data)):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n",
    "        img_adr, img, idmask, _, _ = train_data[i]\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "        idmask_pred, umask_pred, vmask_pred = correspondence_block(img.cuda())\n",
    "        # convert the masks to 240,320 shape\n",
    "        temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "        upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
    "        vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
    "        coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
    "\n",
    "        adr = root_dir + label + \"/predicted_pose/\" + \\\n",
    "            \"info_\" + str(idx) + \".txt\"\n",
    "\n",
    "        coord_2d = torch.cat((coord_2d[0].view(\n",
    "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
    "        dct_keys = tuple(dct_keys.numpy())\n",
    "        dct = load_obj(root_dir + label + \"/UV-XYZ_mapping\")\n",
    "        mapping_2d = []\n",
    "        mapping_3d = []\n",
    "        for count, (u, v) in enumerate(dct_keys):\n",
    "            if (u, v) in dct:\n",
    "                mapping_2d.append(np.array(coord_2d[count]))\n",
    "                mapping_3d.append(dct[(u, v)])\n",
    "        # Get the 6D pose from rotation and translation matrices\n",
    "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
    "        if len(mapping_2d) >= 4 or len(mapping_3d) >= 4:\n",
    "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
    "                                                          np.array(mapping_2d, dtype=np.float32), intrinsic_matrix, distCoeffs=None,\n",
    "                                                          iterationsCount=150, reprojectionError=1.0, flags=cv2.SOLVEPNP_P3P)\n",
    "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
    "            rot[np.isnan(rot)] = 1\n",
    "            tvecs[np.isnan(tvecs)] = 1\n",
    "            tvecs = np.where(-100 < tvecs, tvecs, np.array([-100.]))\n",
    "            tvecs = np.where(tvecs < 100, tvecs, np.array([100.]))\n",
    "            rot_tra = np.append(rot, tvecs, axis=1)\n",
    "            # save the predicted pose\n",
    "            np.savetxt(adr, rot_tra)\n",
    "        else:  # save a pose full of zeros\n",
    "            outliers += 1\n",
    "            rot_tra = np.ones((3, 4))\n",
    "            rot_tra[:, 3] = 0\n",
    "            np.savetxt(adr, rot_tra)\n",
    "    print(\"Number of instances where PnP couldn't be used: \", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_refinement_inputs(root_dir, classes, intrinsic_matrix):\n",
    "    correspondence_block = UNet(\n",
    "        n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n",
    "    correspondence_block.cuda()\n",
    "    correspondence_block.load_state_dict(torch.load(\n",
    "        'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "    train_data = LineMODDataset(root_dir, classes=classes,\n",
    "                                transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    upsampled = nn.Upsample(size=[240, 320], mode='bilinear',align_corners=False)\n",
    "\n",
    "    regex = re.compile(r'\\d+')\n",
    "    count = 0\n",
    "    for i in range(len(train_data)):\n",
    "        if i % 1000 == 0:\n",
    "            print(str(i) + \"/\" + str(len(train_data)) + \" finished!\")\n",
    "        img_adr, img, _, _, _ = train_data[i]\n",
    "\n",
    "        label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "        idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "        adr_rendered = root_dir + label + \\\n",
    "            \"/pose_refinement/rendered/color\" + str(idx) + \".png\"\n",
    "        adr_img = root_dir + label + \\\n",
    "            \"/pose_refinement/real/color\" + str(idx) + \".png\"\n",
    "        # find the object in the image using the idmask\n",
    "        img = img.view(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "        idmask_pred, _, _ = correspondence_block(img.cuda())\n",
    "        idmask = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "        coord_2d = (idmask == classes[label]).nonzero(as_tuple=True)\n",
    "        if coord_2d[0].nelement() != 0:\n",
    "            coord_2d = torch.cat((coord_2d[0].view(\n",
    "                coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "            min_x = coord_2d[:, 0].min()\n",
    "            max_x = coord_2d[:, 0].max()\n",
    "            min_y = coord_2d[:, 1].min()\n",
    "            max_y = coord_2d[:, 1].max()\n",
    "            img = img.squeeze().transpose(1, 2).transpose(0, 2)\n",
    "            obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
    "            # saving in the correct format using upsampling\n",
    "            obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
    "            obj_img = upsampled(obj_img)\n",
    "            obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "            mpimg.imsave(adr_img, obj_img.squeeze().numpy())\n",
    "\n",
    "            # create rendering for an object\n",
    "            cropped_rendered_image = create_rendering(\n",
    "                root_dir, intrinsic_matrix, label, idx)\n",
    "            rendered_img = torch.from_numpy(cropped_rendered_image)\n",
    "            rendered_img = rendered_img.unsqueeze(dim=0)\n",
    "            rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
    "            rendered_img = upsampled(rendered_img)\n",
    "            rendered_img = rendered_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "            mpimg.imsave(adr_rendered, rendered_img.numpy())\n",
    "\n",
    "        else:  # object not present in idmask prediction\n",
    "            count += 1\n",
    "            mpimg.imsave(adr_rendered, np.zeros((240, 320)))\n",
    "            mpimg.imsave(adr_img, np.zeros((240, 320)))\n",
    "    print(\"Number of outliers: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pose_Refiner(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Pose_Refiner, self).__init__()\n",
    "        self.feature_extractor_image = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
    "                                                                           progress=True).children())[:9])\n",
    "        self.feature_extractor_rendered = nn.Sequential(*list(models.resnet18(pretrained=True,\n",
    "                                                                              progress=True).children())[:9])\n",
    "        self.fc_xyhead_1 = nn.Linear(512, 253)\n",
    "        self.fc_xyhead_2 = nn.Linear(256, 2)\n",
    "        self.fc_zhead = nn.Sequential(nn.Linear(512, 256),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(256, 1))\n",
    "        self.fc_Rhead_1 = nn.Linear(512, 252)\n",
    "        self.fc_Rhead_2 = nn.Linear(256, 4)\n",
    "\n",
    "        self.relu_layer = nn.ReLU()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # weight initialization\n",
    "        nn.init.constant_(self.fc_xyhead_1.weight, 0.)\n",
    "        nn.init.constant_(self.fc_xyhead_1.bias, 0.)\n",
    "\n",
    "        weights = torch.zeros((2, 256))\n",
    "        weights[0, 253] = torch.tensor(1.)\n",
    "        weights[1, 254] = torch.tensor(1.)\n",
    "        self.fc_xyhead_2.weight = nn.Parameter(weights)\n",
    "        nn.init.constant_(self.fc_xyhead_2.bias, 0.)\n",
    "\n",
    "        nn.init.constant_(self.fc_zhead.weight, 0.)\n",
    "        nn.init.constant_(self.fc_zhead.bias, 0.)\n",
    "\n",
    "        nn.init.constant_(self.fc_Rhead_1.weight, 0.)\n",
    "        nn.init.constant_(self.fc_Rhead_1.bias, 0.)\n",
    "\n",
    "        rand_weights = torch.zeros((4, 256))\n",
    "        rand_weights[0, 252] = torch.tensor(1.)\n",
    "        rand_weights[1, 253] = torch.tensor(1.)\n",
    "        rand_weights[2, 254] = torch.tensor(1.)\n",
    "        rand_weights[3, 255] = torch.tensor(1.)\n",
    "        self.fc_Rhead_2.weight = nn.Parameter(weights)\n",
    "        nn.init.constant_(self.fc_Rhead_2.bias, 0.)\n",
    "\n",
    "    def forward(self, image, rendered, pred_pose, bs=1):\n",
    "        # extracting the feature vector f\n",
    "        f_image = self.feature_extractor_image(image)\n",
    "        f_rendered = self.feature_extractor_rendered(rendered)\n",
    "        f_image = f_image.view(bs, -1)\n",
    "        f_image = self.relu_layer(f_image)\n",
    "        f_rendered = f_rendered.view(bs, -1)\n",
    "        f_rendered = self.relu_layer(f_rendered)\n",
    "        f = f_image - f_rendered\n",
    "\n",
    "        # Z refinement head\n",
    "        z = self.fc_zhead(f)\n",
    "\n",
    "        # XY refinement head\n",
    "        f_xy1 = self.fc_xyhead_1(f)\n",
    "        f_xy1 = self.relu_layer(f_xy1)\n",
    "        x_pred = np.reshape(pred_pose[:, 0, 3], (bs, -1))\n",
    "        y_pred = np.reshape(pred_pose[:, 1, 3], (bs, -1))\n",
    "        f_xy1 = torch.cat((f_xy1, x_pred.float().cuda()), 1)\n",
    "        f_xy1 = torch.cat((f_xy1, y_pred.float().cuda()), 1)\n",
    "        f_xy1 = torch.cat((f_xy1, z), 1)\n",
    "        xy = self.fc_xyhead_2(f_xy1.cuda())\n",
    "\n",
    "        # Rotation head\n",
    "        f_r1 = self.fc_Rhead_1(f)\n",
    "        f_r1 = self.relu_layer(f_r1)\n",
    "        r = R.from_matrix(pred_pose[:, 0:3, 0:3])\n",
    "        r = r.as_quat()\n",
    "        r = np.reshape(r, (bs, -1))\n",
    "        f_r1 = torch.cat(\n",
    "            (f_r1, torch.from_numpy(r).float().cuda()), 1)\n",
    "        rot = self.fc_Rhead_2(f_r1)\n",
    "\n",
    "        return xy, z, rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rendering(root_dir, intrinsic_matrix, obj, idx):\n",
    "    # helper function to help with creating renderings\n",
    "    pred_pose_adr = root_dir + obj + \\\n",
    "        '/predicted_pose' + '/info_' + str(idx) + \".txt\"\n",
    "    rgb_values = np.loadtxt(root_dir + obj + '/object.xyz',\n",
    "                            skiprows=1, usecols=(6, 7, 8))\n",
    "    coords_3d = np.loadtxt(root_dir + obj + '/object.xyz',\n",
    "                           skiprows=1, usecols=(0, 1, 2))\n",
    "    ones = np.ones((coords_3d.shape[0], 1))\n",
    "    homogenous_coordinate = np.append(coords_3d, ones, axis=1)\n",
    "    rigid_transformation = np.loadtxt(pred_pose_adr)\n",
    "    # Perspective Projection to obtain 2D coordinates\n",
    "    homogenous_2D = intrinsic_matrix @ (\n",
    "        rigid_transformation @ homogenous_coordinate.T)\n",
    "    homogenous_2D[2, :][np.where(homogenous_2D[2, :] == 0)] = 1\n",
    "    coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
    "    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
    "    rendered_image = np.zeros((480, 640, 3))\n",
    "    x_2d = np.clip(coord_2D[:, 0], 0, 479)\n",
    "    y_2d = np.clip(coord_2D[:, 1], 0, 639)\n",
    "    rendered_image[x_2d, y_2d, :] = rgb_values\n",
    "    temp = np.sum(rendered_image, axis=2)\n",
    "    non_zero_indices = np.argwhere(temp > 0)\n",
    "    min_x = non_zero_indices[:, 0].min()\n",
    "    max_x = non_zero_indices[:, 0].max()\n",
    "    min_y = non_zero_indices[:, 1].min()\n",
    "    max_y = non_zero_indices[:, 1].max()\n",
    "    cropped_rendered_image = rendered_image[min_x:max_x +\n",
    "                                            1, min_y:max_y + 1, :]\n",
    "    if cropped_rendered_image.shape[0] > 240 or cropped_rendered_image.shape[1] > 320:\n",
    "        cropped_rendered_image = cv2.resize(np.float32(\n",
    "            cropped_rendered_image), (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    return cropped_rendered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pose_refinement(root_dir, classes, epochs=5):\n",
    "    \n",
    "    train_data = PoseRefinerDataset(root_dir, classes=classes,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        transforms.ToPILImage(mode=None),\n",
    "                                        transforms.Resize(size=(224, 224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [\n",
    "                                                             0.229, 0.224, 0.225]),\n",
    "                                        transforms.ColorJitter(\n",
    "                                            brightness=0, contrast=0, saturation=0, hue=0)\n",
    "                                    ]))\n",
    "\n",
    "    pose_refiner = Pose_Refiner()\n",
    "    pose_refiner.cuda()\n",
    "    # freeze resnet\n",
    "    # pose_refiner.feature_extractor[0].weight.requires_grad = False\n",
    "\n",
    "    batch_size = 2\n",
    "    num_workers = 0\n",
    "    valid_size = 0.2\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # prepare data loaders (combine dataset and sampler)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=train_sampler, num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "    optimizer = optim.Adam(pose_refiner.parameters(),\n",
    "                           lr=3e-4, weight_decay=3e-5)\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = epochs\n",
    "\n",
    "    valid_loss_min = np.Inf  # track change in validation loss\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"----- Epoch Number: \", epoch, \"--------\")\n",
    "\n",
    "        # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        pose_refiner.train()\n",
    "        for label, image, rendered, true_pose, pred_pose in train_loader:\n",
    "            # move tensors to GPU\n",
    "            image, rendered = image.cuda(), rendered.cuda()\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
    "            # convert rot quarternion to rotational matrix\n",
    "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "            rot[rot == float(\"Inf\")] = 1\n",
    "            xy[torch.isnan(xy)] == 0\n",
    "            z[torch.isnan(z)] == 0\n",
    "\n",
    "            rot = torch.tensor(\n",
    "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
    "            # update predicted pose\n",
    "            pred_pose[:, 0:3, 0:3] = rot\n",
    "            pred_pose[:, 0, 3] = xy[:, 0]\n",
    "            pred_pose[:, 1, 3] = xy[:, 1]\n",
    "            pred_pose[:, 2, 3] = z.squeeze()\n",
    "            # fetch point cloud data\n",
    "            pt_cld = fetch_ptcld_data(root_dir, label, batch_size)\n",
    "            # calculate the batch loss\n",
    "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        pose_refiner.eval()\n",
    "        for label, image, rendered, true_pose, pred_pose in valid_loader:\n",
    "            # move tensors to GPU\n",
    "            image, rendered = image.cuda(), rendered.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            xy, z, rot = pose_refiner(image, rendered, pred_pose, batch_size)\n",
    "            rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "            rot[rot == float(\"Inf\")] = 1            \n",
    "            xy[torch.isnan(xy)] == 0\n",
    "            z[torch.isnan(z)] == 0\n",
    "            # convert R quarternion to rotational matrix\n",
    "            rot = torch.tensor(\n",
    "                (R.from_quat(rot.detach().cpu().numpy())).as_matrix())\n",
    "            # update predicted pose\n",
    "            pred_pose[:, 0:3, 0:3] = rot\n",
    "            pred_pose[:, 0, 3] = xy[:, 0]\n",
    "            pred_pose[:, 1, 3] = xy[:, 1]\n",
    "            pred_pose[:, 2, 3] = z.squeeze()\n",
    "            # fetch point cloud data\n",
    "            pt_cld = fetch_ptcld_data(root_dir, label, batch_size)\n",
    "            # calculate the batch loss\n",
    "            loss = Matching_loss(pt_cld, true_pose, pred_pose, batch_size)\n",
    "            # update average validation loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss/len(train_loader.sampler)\n",
    "        valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "\n",
    "        # print training/validation statistics\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_loss, valid_loss))\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min, valid_loss))\n",
    "            torch.save(pose_refiner.state_dict(), 'pose_refiner.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "\n",
    "def fetch_ptcld_data(root_dir, label, bs):\n",
    "    # detch pt cld data for batchsize\n",
    "    pt_cld_data = []\n",
    "    for i in range(bs):\n",
    "        obj_dir = root_dir + label[i] + \"/object.xyz\"\n",
    "        pt_cld = np.loadtxt(obj_dir, skiprows=1, usecols=(0, 1, 2))\n",
    "        index = np.random.choice(pt_cld.shape[0], 3000, replace=False)\n",
    "        pt_cld_data.append(pt_cld[index, :])\n",
    "    pt_cld_data = np.stack(pt_cld_data, axis=0)\n",
    "    return pt_cld_data\n",
    "\n",
    "\n",
    "# no. of points is always 3000\n",
    "def Matching_loss(pt_cld_rand, true_pose, pred_pose, bs, training=True):\n",
    "\n",
    "    total_loss = torch.tensor([0.])\n",
    "    total_loss.requires_grad = True\n",
    "    for i in range(0, bs):\n",
    "        pt_cld = pt_cld_rand[i, :, :].squeeze()\n",
    "        TP = true_pose[i, :, :].squeeze()\n",
    "        PP = pred_pose[i, :, :].squeeze()\n",
    "        target = torch.tensor(pt_cld) @ TP[0:3, 0:3] + torch.cat(\n",
    "            (TP[0, 3].view(-1, 1), TP[1, 3].view(-1, 1), TP[2, 3].view(-1, 1)), 1)\n",
    "        output = torch.tensor(pt_cld) @ PP[0:3, 0:3] + torch.cat(\n",
    "            (PP[0, 3].view(-1, 1), PP[1, 3].view(-1, 1), PP[2, 3].view(-1, 1)), 1)\n",
    "        loss = (torch.abs(output - target).sum())/3000\n",
    "        if loss < 100:\n",
    "            total_loss = total_loss + loss\n",
    "        else:  # so that loss isn't NaN\n",
    "            total_loss = total_loss + torch.tensor([100.])\n",
    "\n",
    "    return total_loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rendering_eval(root_dir, intrinsic_matrix, obj, rigid_transformation):\n",
    "    # helper function to help with creating renderings\n",
    "    rgb_values = np.loadtxt(root_dir + obj + '/object.xyz',\n",
    "                            skiprows=1, usecols=(6, 7, 8))\n",
    "    coords_3d = np.loadtxt(root_dir + obj + '/object.xyz',\n",
    "                           skiprows=1, usecols=(0, 1, 2))\n",
    "    ones = np.ones((coords_3d.shape[0], 1))\n",
    "    homogenous_coordinate = np.append(coords_3d, ones, axis=1)\n",
    "    # Perspective Projection to obtain 2D coordinates\n",
    "    homogenous_2D = intrinsic_matrix @ (\n",
    "        rigid_transformation @ homogenous_coordinate.T)\n",
    "    homogenous_2D[2, :][np.where(homogenous_2D[2, :] == 0)] = 1\n",
    "    coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n",
    "    coord_2D = ((np.floor(coord_2D)).T).astype(int)\n",
    "    rendered_img = np.zeros((480, 640, 3))\n",
    "    x_2d = np.clip(coord_2D[:, 0], 0, 479)\n",
    "    y_2d = np.clip(coord_2D[:, 1], 0, 639)\n",
    "    rendered_img[x_2d, y_2d, :] = rgb_values\n",
    "    temp = np.sum(rendered_img, axis=2)\n",
    "    non_zero_indices = np.argwhere(temp > 0)\n",
    "    min_x = non_zero_indices[:, 0].min()\n",
    "    max_x = non_zero_indices[:, 0].max()\n",
    "    min_y = non_zero_indices[:, 1].min()\n",
    "    max_y = non_zero_indices[:, 1].max()\n",
    "    cropped_rendered_img = rendered_img[min_x:max_x +\n",
    "                                        1, min_y:max_y + 1, :]\n",
    "    if cropped_rendered_img.shape[0] > 240 or cropped_rendered_img.shape[1] > 320:\n",
    "        cropped_rendered_img = cv2.resize(np.float32(\n",
    "            cropped_rendered_img), (320, 240), interpolation=cv2.INTER_AREA)\n",
    "    return cropped_rendered_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"LineMOD_Dataset\")\n",
    "    file1 = open('dataset_install.txt', 'r') \n",
    "    Lines = file1.readlines()\n",
    "    for url in Lines[:-1]:\n",
    "        zipresp = urlopen(url)\n",
    "        tempzip = open(\"tempfile.zip\", \"wb\")\n",
    "        tempzip.write(zipresp.read())\n",
    "        tempzip.close()\n",
    "\n",
    "        zf = ZipFile(\"tempfile.zip\")\n",
    "        zf.extractall(path = 'LineMOD_Dataset')\n",
    "        zf.close()\n",
    "        \n",
    "    zipresp = urlopen(Lines[-1])\n",
    "    tempzip = open(\"tempfile.zip\", \"wb\")\n",
    "    tempzip.write(zipresp.read())\n",
    "    tempzip.close()\n",
    "\n",
    "    zf = ZipFile(\"tempfile.zip\")\n",
    "    zf.extractall()\n",
    "    zf.close()\n",
    "except FileExistsError:\n",
    "    print(\"Data set exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Script to create the Ground Truth masks')\n",
    "parser.add_argument(\"--root_dir\", default=\"LineMOD_Dataset/\",\n",
    "                    help=\"path to dataset directory\")\n",
    "\n",
    "parser.add_argument(\"--bgd_dir\", default=\"val2017/\",\n",
    "                    help=\"path to background images dataset directory\")\n",
    "parser.add_argument(\"--split\", default=0.05, help=\"train:test split ratio\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 790\n",
      "Testing Samples: 15011\n"
     ]
    }
   ],
   "source": [
    "root_dir = args.root_dir\n",
    "background_dir = args.bgd_dir\n",
    "\n",
    "list_all_images = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):  # images that exist\n",
    "            list_all_images.append(os.path.join(root, file))\n",
    "\n",
    "num_images = len(list_all_images)\n",
    "indices = list(range(num_images))\n",
    "np.random.seed(69)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(args.split * num_images))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "print(\"Training Samples:\", len(train_idx))\n",
    "print(\"Testing Samples:\", len(test_idx))\n",
    "\n",
    "save_obj(list_all_images, root_dir + \"all_images_adr\")\n",
    "save_obj(train_idx, root_dir + \"train_images_indices\")\n",
    "save_obj(test_idx, root_dir + \"test_images_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories already exist\n"
     ]
    }
   ],
   "source": [
    "classes = {'ape': 1, \n",
    "           'benchviseblue': 2, \n",
    "           'cam': 3, \n",
    "           'can': 4, \n",
    "           'cat': 5, \n",
    "           'driller': 6,\n",
    "           'duck': 7, \n",
    "           'eggbox': 8, \n",
    "           'glue': 9, \n",
    "           'holepuncher': 10, \n",
    "           'iron': 11, \n",
    "           'lamp': 12, \n",
    "           'phone': 13}\n",
    "class_names = list(classes.keys())\n",
    "dataset_dir_structure(root_dir, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 572.41140\n",
    "px = 325.26110\n",
    "fy = 573.57043\n",
    "py = 242.04899\n",
    "intrinsic_matrix = np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Start creating ground truth ------\n",
      "0/790 finished!\n",
      "----- Finished creating ground truth -----\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Start creating ground truth ------\")\n",
    "create_GT_masks(root_dir, background_dir, intrinsic_matrix, classes)\n",
    "create_UV_XYZ_dictionary(root_dir)  # create UV - XYZ dictionaries\n",
    "print(\"----- Finished creating ground truth -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Started training of the correspondence block ------\n",
      "------ Epoch  1  ---------\n",
      "Epoch: 1 \tTraining Loss: 1.004504 \tValidation Loss: 0.697700\n",
      "Validation loss decreased (inf --> 0.697700).  Saving model ...\n",
      "------ Epoch  2  ---------\n",
      "Epoch: 2 \tTraining Loss: 0.522822 \tValidation Loss: 0.680501\n",
      "Validation loss decreased (0.697700 --> 0.680501).  Saving model ...\n",
      "------ Epoch  3  ---------\n",
      "Epoch: 3 \tTraining Loss: 0.504254 \tValidation Loss: 0.690435\n",
      "------ Epoch  4  ---------\n",
      "Epoch: 4 \tTraining Loss: 0.470434 \tValidation Loss: 0.683854\n",
      "------ Epoch  5  ---------\n",
      "Epoch: 5 \tTraining Loss: 0.448585 \tValidation Loss: 0.586862\n",
      "Validation loss decreased (0.680501 --> 0.586862).  Saving model ...\n",
      "------ Epoch  6  ---------\n",
      "Epoch: 6 \tTraining Loss: 0.436281 \tValidation Loss: 0.589098\n",
      "------ Epoch  7  ---------\n",
      "Epoch: 7 \tTraining Loss: 0.427105 \tValidation Loss: 0.715638\n",
      "------ Epoch  8  ---------\n",
      "Epoch: 8 \tTraining Loss: 0.407672 \tValidation Loss: 0.585794\n",
      "Validation loss decreased (0.586862 --> 0.585794).  Saving model ...\n",
      "------ Epoch  9  ---------\n",
      "Epoch: 9 \tTraining Loss: 0.389014 \tValidation Loss: 0.512378\n",
      "Validation loss decreased (0.585794 --> 0.512378).  Saving model ...\n",
      "------ Epoch  10  ---------\n",
      "Epoch: 10 \tTraining Loss: 0.386046 \tValidation Loss: 0.574950\n",
      "------ Epoch  11  ---------\n",
      "Epoch: 11 \tTraining Loss: 0.370671 \tValidation Loss: 0.571702\n",
      "------ Epoch  12  ---------\n",
      "Epoch: 12 \tTraining Loss: 0.363986 \tValidation Loss: 0.590624\n",
      "------ Epoch  13  ---------\n",
      "Epoch: 13 \tTraining Loss: 0.357685 \tValidation Loss: 0.479514\n",
      "Validation loss decreased (0.512378 --> 0.479514).  Saving model ...\n",
      "------ Epoch  14  ---------\n",
      "Epoch: 14 \tTraining Loss: 0.346377 \tValidation Loss: 0.584056\n",
      "------ Epoch  15  ---------\n",
      "Epoch: 15 \tTraining Loss: 0.339963 \tValidation Loss: 0.571834\n",
      "------ Epoch  16  ---------\n",
      "Epoch: 16 \tTraining Loss: 0.340432 \tValidation Loss: 0.573634\n",
      "------ Epoch  17  ---------\n",
      "Epoch: 17 \tTraining Loss: 0.335966 \tValidation Loss: 0.469885\n",
      "Validation loss decreased (0.479514 --> 0.469885).  Saving model ...\n",
      "------ Epoch  18  ---------\n",
      "Epoch: 18 \tTraining Loss: 0.328688 \tValidation Loss: 0.446494\n",
      "Validation loss decreased (0.469885 --> 0.446494).  Saving model ...\n",
      "------ Epoch  19  ---------\n",
      "Epoch: 19 \tTraining Loss: 0.310691 \tValidation Loss: 0.479640\n",
      "------ Epoch  20  ---------\n",
      "Epoch: 20 \tTraining Loss: 0.314302 \tValidation Loss: 0.515863\n",
      "-------------------- Training Finished -------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------ Started training of the correspondence block ------\")\n",
    "torch.cuda.empty_cache()\n",
    "train_correspondence_block(root_dir, classes, epochs=20, batch_size=1)\n",
    "print(\"-------------------- Training Finished -------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Started Initial pose estimation ------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.24 GiB already allocated; 6.63 MiB free; 4.26 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-56b31daf7915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------ Started Initial pose estimation ------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minitial_pose_estimation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintrinsic_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"------ Finished Initial pose estimation -----\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a7e25942856d>\u001b[0m in \u001b[0;36minitial_pose_estimation\u001b[1;34m(root_dir, classes, intrinsic_matrix)\u001b[0m\n\u001b[0;32m      8\u001b[0m     correspondence_block = UNet(\n\u001b[0;32m      9\u001b[0m         n_channels=3, out_channels_id=14, out_channels_uv=256, bilinear=True)\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcorrespondence_block\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     correspondence_block.load_state_dict(torch.load(\n\u001b[0;32m     12\u001b[0m         'correspondence_block.pt', map_location=torch.device('cpu')))\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Install\\Anaconda\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 4.24 GiB already allocated; 6.63 MiB free; 4.26 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "print(\"------ Started Initial pose estimation ------\")\n",
    "torch.cuda.empty_cache()\n",
    "initial_pose_estimation(root_dir, classes, intrinsic_matrix)\n",
    "print(\"------ Finished Initial pose estimation -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----- Started creating inputs for DL based pose refinement ------\")\n",
    "create_refinement_inputs(root_dir, classes, intrinsic_matrix)\n",
    "print(\"----- Finished creating inputs for DL based pose refinement -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Started training DL based pose refiner ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\naikp/.cache\\torch\\checkpoints\\resnet18-5c106cde.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch Number:  1 --------\n",
      "Epoch: 1 \tTraining Loss: 55.934628 \tValidation Loss: 39.522605\n",
      "Validation loss decreased (inf --> 39.522605).  Saving model ...\n",
      "----- Epoch Number:  2 --------\n",
      "Epoch: 2 \tTraining Loss: 35.890051 \tValidation Loss: 79.330465\n",
      "----- Epoch Number:  3 --------\n",
      "Epoch: 3 \tTraining Loss: 29.590744 \tValidation Loss: 49.013834\n",
      "----- Epoch Number:  4 --------\n",
      "Epoch: 4 \tTraining Loss: 28.720850 \tValidation Loss: 52.118059\n",
      "----- Epoch Number:  5 --------\n",
      "Epoch: 5 \tTraining Loss: 28.327229 \tValidation Loss: 43.019825\n",
      "----- Epoch Number:  6 --------\n",
      "Epoch: 6 \tTraining Loss: 28.289032 \tValidation Loss: 41.691128\n",
      "----- Epoch Number:  7 --------\n",
      "Epoch: 7 \tTraining Loss: 28.361676 \tValidation Loss: 42.929724\n",
      "----- Epoch Number:  8 --------\n",
      "Epoch: 8 \tTraining Loss: 28.092650 \tValidation Loss: 54.804329\n",
      "----- Epoch Number:  9 --------\n",
      "Epoch: 9 \tTraining Loss: 27.791432 \tValidation Loss: 67.711076\n",
      "----- Epoch Number:  10 --------\n",
      "Epoch: 10 \tTraining Loss: 28.364546 \tValidation Loss: 49.172935\n",
      "----- Finished training DL based pose refiner ------\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Started training DL based pose refiner ------\")\n",
    "train_pose_refinement(root_dir, classes, epochs=10)\n",
    "print(\"----- Finished training DL based pose refiner ------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-8b79ee6198a2>:13: RuntimeWarning: overflow encountered in true_divide\n",
      "  coord_2D = homogenous_2D[:2, :] / homogenous_2D[2, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD Score for all testing images is:  0.5423356205449337\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='Script to create the Ground Truth masks')\n",
    "parser.add_argument(\"--root_dir\", default=\"LineMOD_Dataset/\",\n",
    "                    help=\"path to dataset directory\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "root_dir = args.root_dir\n",
    "\n",
    "classes = {'ape': 1, 'benchviseblue': 2, 'cam': 3, 'can': 4, 'cat': 5, 'driller': 6,\n",
    "           'duck': 7, 'eggbox': 8, 'glue': 9, 'holepuncher': 10, 'iron': 11, 'lamp': 12, 'phone': 13}\n",
    "\n",
    "score_card = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
    "              'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
    "\n",
    "instances = {'ape': 0, 'benchviseblue': 0, 'cam': 0, 'can': 0, 'cat': 0, 'driller': 0,\n",
    "             'duck': 0, 'eggbox': 0, 'glue': 0, 'holepuncher': 0, 'iron': 0, 'lamp': 0, 'phone': 0}\n",
    "\n",
    "transform = transforms.Compose([transforms.ToPILImage(mode=None),\n",
    "                                transforms.Resize(size=(224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "fx = 572.41140\n",
    "px = 325.26110\n",
    "fy = 573.57043\n",
    "py = 242.04899  # Intrinsic Parameters of the Camera\n",
    "intrinsic_matrix = np.array([[fx, 0, px], [0, fy, py], [0, 0, 1]])\n",
    "\n",
    "correspondence_block = UNet(n_channels=3, out_channels_id=14,\n",
    "                                 out_channels_uv=256, bilinear=True)\n",
    "# load the best weights from the training loop\n",
    "correspondence_block.load_state_dict(torch.load(\n",
    "    'correspondence_block.pt', map_location=torch.device('cpu')))\n",
    "pose_refiner = Pose_Refiner()\n",
    "# load the best weights from the training loop\n",
    "pose_refiner.load_state_dict(torch.load(\n",
    "    'pose_refiner.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "correspondence_block.cuda()\n",
    "pose_refiner.cuda()\n",
    "pose_refiner.eval()\n",
    "correspondence_block.eval()\n",
    "\n",
    "list_all_images = load_obj(root_dir + \"all_images_adr\")\n",
    "testing_images_idx = load_obj(root_dir + \"test_images_indices\")\n",
    "\n",
    "regex = re.compile(r'\\d+')\n",
    "upsampled = nn.Upsample(size=[240, 320], mode='bilinear', align_corners=False)\n",
    "total_score = 0\n",
    "for i in range(len(testing_images_idx)):\n",
    "\n",
    "    img_adr = list_all_images[testing_images_idx[i]]\n",
    "    label = os.path.split(os.path.split(os.path.dirname(img_adr))[0])[1]\n",
    "    idx = regex.findall(os.path.split(img_adr)[1])[0]\n",
    "\n",
    "    tra_adr = root_dir + label + \"/data/tra\" + str(idx) + \".tra\"\n",
    "    rot_adr = root_dir + label + \"/data/rot\" + str(idx) + \".rot\"\n",
    "    true_pose = get_rot_tra(rot_adr, tra_adr)\n",
    "\n",
    "    test_img = cv2.imread(img_adr)\n",
    "    test_img = cv2.resize(\n",
    "        test_img, (test_img.shape[1]//2, test_img.shape[0]//2), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    test_img = torch.from_numpy(test_img).type(torch.double)\n",
    "    test_img = test_img.transpose(1, 2).transpose(0, 1)\n",
    "\n",
    "    if len(test_img.shape) != 4:\n",
    "        test_img = test_img.view(\n",
    "            1, test_img.shape[0], test_img.shape[1], test_img.shape[2])\n",
    "\n",
    "    # pass through correspondence block\n",
    "    idmask_pred, umask_pred, vmask_pred = correspondence_block(\n",
    "        test_img.float().cuda())\n",
    "\n",
    "    # convert the masks to 240,320 shape\n",
    "    temp = torch.argmax(idmask_pred, dim=1).squeeze().cpu()\n",
    "    upred = torch.argmax(umask_pred, dim=1).squeeze().cpu()\n",
    "    vpred = torch.argmax(vmask_pred, dim=1).squeeze().cpu()\n",
    "    coord_2d = (temp == classes[label]).nonzero(as_tuple=True)\n",
    "    if coord_2d[0].nelement() != 0:  # label is detected in the image\n",
    "        coord_2d = torch.cat((coord_2d[0].view(\n",
    "            coord_2d[0].shape[0], 1), coord_2d[1].view(coord_2d[1].shape[0], 1)), 1)\n",
    "        uvalues = upred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        vvalues = vpred[coord_2d[:, 0], coord_2d[:, 1]]\n",
    "        dct_keys = torch.cat((uvalues.view(-1, 1), vvalues.view(-1, 1)), 1)\n",
    "        dct_keys = tuple(dct_keys.numpy())\n",
    "        dct = load_obj(root_dir + label + \"/UV-XYZ_mapping\")\n",
    "        mapping_2d = []\n",
    "        mapping_3d = []\n",
    "        for count, (u, v) in enumerate(dct_keys):\n",
    "            if (u, v) in dct:\n",
    "                mapping_2d.append(np.array(coord_2d[count]))\n",
    "                mapping_3d.append(dct[(u, v)])\n",
    "\n",
    "        # PnP needs atleast 6 unique 2D-3D correspondences to run\n",
    "        if len(mapping_2d) >= 6 or len(mapping_3d) >= 6:\n",
    "            _, rvecs, tvecs, inliers = cv2.solvePnPRansac(np.array(mapping_3d, dtype=np.float32),\n",
    "                                                          np.array(mapping_2d, dtype=np.float32), intrinsic_matrix, distCoeffs=None,\n",
    "                                                          iterationsCount=150, reprojectionError=1.0, flags=cv2.SOLVEPNP_P3P)\n",
    "            rot, _ = cv2.Rodrigues(rvecs, jacobian=None)\n",
    "            pred_pose = np.append(rot, tvecs, axis=1)\n",
    "\n",
    "        else:  # save an empty file\n",
    "            pred_pose = np.zeros((3, 4))\n",
    "\n",
    "        min_x = coord_2d[:, 0].min()\n",
    "        max_x = coord_2d[:, 0].max()\n",
    "        min_y = coord_2d[:, 1].min()\n",
    "        max_y = coord_2d[:, 1].max()\n",
    "        img = test_img.squeeze().transpose(1, 2).transpose(0, 2)\n",
    "        obj_img = img[min_x:max_x+1, min_y:max_y+1, :]\n",
    "        # saving in the correct format using upsampling\n",
    "        obj_img = obj_img.transpose(0, 1).transpose(0, 2).unsqueeze(dim=0)\n",
    "        obj_img = upsampled(obj_img)\n",
    "        obj_img = obj_img.squeeze().transpose(0, 2).transpose(0, 1)\n",
    "        obj_img = transform(torch.as_tensor(obj_img, dtype=torch.float32))\n",
    "        # create rendering for an object\n",
    "        cropped_rendered_img = create_rendering_eval(\n",
    "            root_dir, intrinsic_matrix, label, pred_pose)\n",
    "        rendered_img = torch.from_numpy(cropped_rendered_img)\n",
    "        rendered_img = rendered_img.unsqueeze(dim=0)\n",
    "        rendered_img = rendered_img.transpose(1, 3).transpose(2, 3)\n",
    "        rendered_img = upsampled(rendered_img)\n",
    "        rendered_img = rendered_img.squeeze()\n",
    "        rendered_img = transform(torch.as_tensor(\n",
    "            rendered_img, dtype=torch.float32))\n",
    "\n",
    "        if len(rendered_img.shape) != 4:\n",
    "            rendered_img = rendered_img.view(\n",
    "                1, rendered_img.shape[0], rendered_img.shape[1], rendered_img.shape[2])\n",
    "\n",
    "        if len(obj_img.shape) != 4:\n",
    "            obj_img = obj_img.view(\n",
    "                1, obj_img.shape[0], obj_img.shape[1],  obj_img.shape[2])\n",
    "        pred_pose = (torch.from_numpy(pred_pose)).unsqueeze(0)\n",
    "\n",
    "        # pose refinement to get final output\n",
    "        xy, z, rot = pose_refiner(obj_img.float().cuda(),\n",
    "                                  rendered_img.float().cuda(), pred_pose)\n",
    "        # below 2 lines are for outliers only - edge case                          \n",
    "        rot[torch.isnan(rot)] = 1  # take care of NaN and inf values\n",
    "        rot[rot == float(\"Inf\")] = 1\n",
    "        xy[torch.isnan(xy)] = 0\n",
    "        z[torch.isnan(z)] = 0\n",
    "        # convert R quarternion to rotational matrix\n",
    "        rot = (R.from_quat(rot.detach().cpu().numpy())).as_matrix()\n",
    "        pred_pose = pred_pose.squeeze().numpy()\n",
    "        # update predicted pose\n",
    "        xy = xy.squeeze()\n",
    "        pred_pose[0:3, 0:3] = rot\n",
    "        pred_pose[0, 3] = xy[0]\n",
    "        pred_pose[1, 3] = xy[1]\n",
    "        pred_pose[2, 3] = z\n",
    "\n",
    "        diameter = np.loadtxt(root_dir + label + \"/distance.txt\")\n",
    "        ptcld_file = root_dir + label + \"/object.xyz\"\n",
    "        pt_cld = np.loadtxt(ptcld_file, skiprows=1, usecols=(0, 1, 2))\n",
    "        test_rendered = create_bounding_box(img_for_bounding_box, pred_pose, pt_cld, intrinsic_matrix)\n",
    "        test_rendered = create_bounding_box(bounded_img, true_pose, pt_cld, intrinsic_matrix, color = (0,255,0))\n",
    "\n",
    "        score = ADD_score(pt_cld, true_pose, pred_pose, diameter)\n",
    "        path = str(score) + \"_score_\" + str(i) +'.png'\n",
    "        cv2.imwrite(path, test_rendered)\n",
    "        total_score += score\n",
    "        score_card[label] += score\n",
    "\n",
    "    else:\n",
    "        score_card[label] += 0\n",
    "\n",
    "    instances[label] += 1\n",
    "\n",
    "\n",
    "print(\"ADD Score for all testing images is: \",\n",
    "      total_score/len(testing_images_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
